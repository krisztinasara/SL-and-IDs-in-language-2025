---
editor_options: 
  markdown: 
    wrap: 72
---

|                                                                                                                                                                                                        |
|------------------------------------------------------------------------|
| title: "Statistical learning and individual differences in language abilities: A structural equation modelling study on the mediating roles of perceptual speed, working memory and cognitive control" |
| output: html_notebook                                                                                                                                                                                  |

# Preparation

## R packages:

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(psych)
library(EFAtools)
library(lavaan)
library(ClustOfVar)
library(semTools)
library(semPlot)
library(tidySEM)
```

## Data:

```{r include = FALSE, warning = FALSE}
library(readr)
df = read_delim("df.csv", 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)
```

```{r message = FALSE, warning = FALSE}
df = df %>% 
  mutate(
    perceptual_speed_visual_RT = -(perceptual_speed_visual_RT),
    perceptual_speed_visual_decision = -(perceptual_speed_visual_decision),
    perceptual_speed_auditory_RT = -(perceptual_speed_auditory_RT),
    simon_RT = -(simon_RT),
    simon_accuracy = -(simon_accuracy),
    stroop_RT = -(stroop_RT),
    stroop_accuracy = -(stroop_accuracy),
    selfpaced_gardenpath_target = -(selfpaced_gardenpath_target),
    selfpaced_violation_processing_target = -(selfpaced_violation_processing_target)
  )
```

# 1. Selecting reliable indices

Our initial data set comprised 12 tasks with 45 indices (Table 1). Each
index has a minimum overlap of 100 cases with each other indices. For
each index, two criteria were considered: (i) **unidimensionality** and
(ii) **internal consistency**, indicative of its reliability. Indices
failing to meet either criterion were dropped. Indices that could not be
evaluated were retained under the assumption that they might contain
valuable information for the overall model.

Unidimensionality was assessed using Exploratory Factor Analysis (EFA)
on individual items within the task index. Internal consistency was
evaluated using the split-half method or omega coefficient where
applicable, with a minimum reliability level set at 0.5 as the inclusion
criterion.

For *grammatical sensitivity* and *pragmatic comprehension* indices, the
item composition was modified to meet the criteria. Since these two
indices consist of qualitatively different items, to prevent
overfitting, we divided the sample into two halves. One half was used to
select well-fitting items in the modified index, and the other half was
employed to evaluate the new structure.

Consequently, 13 indices were dropped, leaving 32 indices introduced to
the EFA models.

**Table 1**

*Reliability of indices of the original dataset*

| group | task                           | index                                        | reliability type | reliability value | unidimensionality | suitable |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| COG   | perceptual speed               | visual reaction time                         | split-half\*     | 0.957             | n.a.              | yes      |
| COG   | perceptual speed               | auditory reaction time                       | split-half\*     | 0.956             | n.a.              | yes      |
| COG   | perceptual speed               | visual decision score                        | split-half\*     | 0.179             | n.a.              | no       |
| COG   | perceptual speed               | visual decision time                         | split-half\*     | 0.952             | n.a.              | yes      |
| COG   | simon                          | simon RT                                     | split-half\*     | 0.643             | n.a.              | yes      |
| COG   | simon                          | simon accuracy                               | split-half\*     | 0.546             | n.a.              | yes      |
| COG   | stroop                         | stroop RT                                    | split-half\*     | 0.768             | n.a.              | yes      |
| COG   | stroop                         | stroop accuracy                              | split-half\*     | 0.876             | n.a.              | yes      |
| COG   | digit span                     | forward digit span                           | n.a.             | n.a.              | n.a.              | yes      |
| COG   | digit span                     | backward digit span                          | n.a.             | n.a.              | n.a.              | yes      |
| COG   | n-back                         | 1-back d-prime                               | split-half\*     | 0.807             | n.a.              | yes      |
| COG   | n-back                         | 2-back d-prime                               | split-half\*     | 0.763             | n.a.              | yes      |
| COG   | n-back                         | 3-back d-prime                               | split-half\*     | 0.691             | n.a.              | yes      |
| SL    | speech segmentation            | SEGM RT training                             | split-half\*     | 0.646             | n.a.              | yes      |
| SL    | speech segmentation            | SEGM RT TRN3-RND4                            | split-half\*     | 0.768             | n.a.              | yes      |
| SL    | speech segmentation            | SEGM RT RND4-REC5                            | split-half\*     | 0.778             | n.a.              | yes      |
| SL    | speech segmentation            | SEGM ACC training                            | split-half\*     | 0.855             | n.a.              | yes      |
| SL    | speech segmentation            | SEGM ACC TRN3-RND4                           | split-half\*     | 0.850             | n.a.              | yes      |
| SL    | speech segmentation            | SEGM ACC RND4-REC5                           | split-half\*     | 0.761             | n.a.              | yes      |
| SL    | speech segmentation            | SEGM 2AFC bigram                             | n.a.             | n.a.              | n.a.              | yes      |
| SL    | speech segmentation            | SEGM 2AFC trigram                            | n.a.             | n.a.              | n.a.              | yes      |
| SL    | speech segmentation            | SEGM production                              | n.a.             | n.a.              | n.a.              | yes      |
| SL    | artificial grammar learning    | AGL RT training                              | split-half\*     | 0.222             | n.a.              | no       |
| SL    | artificial grammar learning    | AGL RT TRN3-RND4                             | split-half\*     | 0.270             | n.a.              | no       |
| SL    | artificial grammar learning    | AGL RT RND4-REC5                             | split-half\*     | 0.247             | n.a.              | no       |
| SL    | artificial grammar learning    | AGL ACC training                             | split-half\*     | 0.634             | n.a.              | yes      |
| SL    | artificial grammar learning    | AGL ACC TRN3-RND4                            | split-half\*     | 0.380             | n.a.              | no       |
| SL    | artificial grammar learning    | AGL ACC RND4-REC5                            | split-half\*     | 0.390             | n.a.              | no       |
| SL    | artificial grammar learning    | AGL 2AFC sentence                            | n.a.             | n.a.              | n.a.              | yes      |
| SL    | artificial grammar learning    | AGL 2AFC phrase                              | n.a.             | n.a.              | n.a.              | yes      |
| SL    | artificial grammar learning    | AGL production                               | n.a.             | n.a.              | n.a.              | yes      |
| LANG  | predictive sentence processing | predictive sentence processing RT            | split-half\*     | 0.853             | n.a.              | yes      |
| LANG  | predictive sentence processing | predictive sentence processing accuracy      | split-half\*     | 0.585             | n.a.              | yes      |
| LANG  | KOBAK                          | pragmatic comprehension                      | omega            | 0.630             | 1                 | yes      |
| LANG  | grammatical sensitivity        | grammatical sensitivity                      | omega            | 0.676             | 1                 | yes      |
| LANG  | self-paced reading             | garden path - target                         | omega            | 0.506             | 1                 | yes      |
| LANG  | self-paced reading             | garden path - plus one                       | omega            | -0.060            | 0                 | no       |
| LANG  | self-paced reading             | past participle garden path - target         | omega            | 0.270             | 0                 | no       |
| LANG  | self-paced reading             | past participle garden path - plus one       | omega            | 0.140             | 0                 | no       |
| LANG  | self-paced reading             | object relative clause processing - target   | omega            | -0.030            | 1                 | no       |
| LANG  | self-paced reading             | object relative clause processing - plus one | omega            | 0.576             | 0                 | no       |
| LANG  | self-paced reading             | sentence violation processing - target       | omega            | 0.681             | 1                 | yes      |
| LANG  | self-paced reading             | sentence violation processing - plus one     | omega            | 0.391             | 0                 | no       |
| LANG  | one-minute reading             | one-minute reading                           | n.a.             | n.a.              | n.a.              | yes      |

*Note.* \*: Reported split-half reliabilities are the mean of 100
iterations, Spearman-Brown corrected at each iteration.

# 2. Latent factor identification

## 2.1. Summary for the process of latent factor identification

(See details below)

To identify latent factors, we initially categorised the measurement
indices into three designated domains: the *statistical learning* (SL)
domain, the *core cognitive* (COG) domain, and the *language* (LANG)
domain (see Table 1).

As a next step, exploratory factor analyses (EFAs) were conducted within
each domain, following these procedures:

1.  **KMO test**: Using the Kaiser-Meyer-Olkin test, we assessed the
    measures of sampling adequacy (MSA) for the indices. Subsequently,
    indices with unsatisfactory fit for EFA (MSA \< 0.6) were excluded.

2.  **Bartlett's test**: Secondly, Bartlett's test of sphericity was
    employed, with the significance level indicating a suitable fit for
    EFA.

3.  **Parallel analysis**: To determine the number of factors in each
    domain, Horn's parallel analysis was used. This method compares
    eigenvalues derived from the data matrix to those from a Monte Carlo
    simulated matrix generated from random data of the same size.

4.  **Exploratory factor analysis**: After determining the number of
    factors, exploratory factor analysis was conducted using maximum
    likelihood as the factoring method and promax (oblique) rotation.
    Missing values were imputed using the mean. Based on factor loadings
    and uniqueness observed in each index, we refined our model.

5.  **Hierarchical cluster analysis**: To validate EFA results,
    hierarchical cluster analyses were performed. The stability of
    partitions from a hierarchy of variables was evaluated using a
    bootstrapping method (n=100). Based on the mean adjusted Rand
    criterion, we examined local maximums regarding the potential number
    of clusters and selected the one supported by our theoretical
    considerations if necessary. We then proceeded with k-means
    clustering of variables, resulting in the identification of latent
    variables.

6.  **Confirmatory factor analysis**: In the final step, we validated
    the structure through confirmatory factor analysis. Analyses were
    conducted using the Robust Maximum Likelihood method, with missing
    values imputed by the Maximum Likelihood method. Final models were
    refined if necessary.

During these steps, seven indices were removed from the initial 32
indices for not meeting the KMO test criterion, and two additional
indices were dropped due to model refinement. Thus, the final data set
included 23 indices from 12 different tasks (Table 2).

## 2.2. Latent factors in statistical learning domain

**Kaiser-Meyer-Olkin test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_ACC_training,
    SEGM_ACC_TRN3_RND4,
    SEGM_ACC_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_ACC_training,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
KMO()
```

Indices with unsatisfactory fit for EFA (MSA \< 0.5) were excluded: -
SEGM_ACC_training [speech segmentation - SEGM ACC training] -
SEGM_ACC_TRN3_RND4 [speech segmentation - SEGM ACC TRN3-RND4] -
SEGM_ACC_RND4_REC5 [speech segmentation - SEGM ACC RND4-REC5] -
AGL_ACC_training [artificial grammar learning - AGL ACC training]

Then, KMO test was repeated.

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
KMO()
```

The reaction time based indices showed poor fit (\< 0.6), however, we
did not omit them for their possible exploratory power. Instead, we
performed two separate analysis with only the reaction time based
variables and the remaining ones.

### 2.2.1. RT variables

**Bartlett's test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates a single factor.

**Exploratory factor analysis**:

```{r echo = FALSE, warning = FALSE}
SL_RT = df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 1,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r echo = FALSE, warning = FALSE}
summary(SL_RT)
```

```{r echo = FALSE, warning = FALSE}
SL_RT$loadings
tidy(SL_RT$uniquenesses)
```

### 2.2.2. Offline SL indices (non-RT indices)

**Bartlett's test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates 2 factors.

**Exploratory factor analysis**:

```{r echo = FALSE, warning = FALSE}
SL_OFF = df %>% 
  select(
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 2,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r echo = FALSE, warning = FALSE}
summary(SL_OFF)
```

```{r echo = FALSE, warning = FALSE}
SL_OFF$loadings
tidy(SL_OFF$uniquenesses)
```

### 2.2.3. Final model for statistical learning variables

Since the 2-factor model showed a complex structure, we proceeded with
hierarchical cluster analysis to reveal potential single-variable
factors.

**Hierarchical cluster analysis**:

```{r echo = FALSE, warning = FALSE}
tree_01 = data.frame(df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  )) %>% 
hclustvar()
```

```{r echo = FALSE, warning = FALSE}
plot(tree_01)
```

```{r echo = FALSE, warning = FALSE}
stab = stability(tree_01, B = 100)
```

Since the Rand criterion shows that there is two local maxima (at 3 and
5) regarding the number of clusters, we selected the greater number to
get a more detailed analysis.

**K-means cluster analysis**:

```{r echo = FALSE, warning = FALSE}
km = data.frame(df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  )) %>% 
kmeansvar(init = 5, nstart = 1000)
```

```{r echo = FALSE, warning = FALSE}
summary(km)
```

In conclusion, we used these 5 clusters of statistical learning
variables to identify latent factors which later serve as predictors
in the SEM models. We tested the robustness of the results with
confirmatory factor analysis.

**Confirmatory factor analysis**:

```{r echo = FALSE, warning = FALSE}
model_SL =
"
  SL_RT =~ SEGM_RT_training + SEGM_RT_TRN3_RND4 + SEGM_RT_RND4_REC5
  SL_tribi =~ SEGM_2AFC_bigram + SEGM_2AFC_trigram
  SL_SEGM_prod =~ SEGM_production
  SL_AGL =~ AGL_2AFC_sentence + AGL_production
  SL_phr =~ AGL_2AFC_phrase
"
```

**Model fits for CFA**:

```{r echo = FALSE, warning = FALSE}
model_SL_fit = cfa(model_SL, df, missing = "ML", estimator = "MLR")
summary(model_SL_fit, standardized = TRUE, fit.measures = TRUE)
```

In one case (*SEGM_2AFC_bigram [speech segmentation - SEGM 2AFC
bigram]*), the unexplained variance was high (\>0.65), however we kept
it to assign convergent validity to the SL_trigram variable. We further
excluded the *SEGM_AL_midRT_train [speech segmentation - SEGM RT
training]* index to solve the Heywood case present in this factor.

------------------------------------------------------------------------

The final grouping of the variables is:

1.  **SEGM RT**:

-   SEGM_RT_TRN3_RND4 [speech segmentation - SEGM RT TRN3-RND4]
-   SEGM_RT_RND4_REC5 [speech segmentation - SEGM RT RND4-REC5]

2.  **SEGM 2AFC**:

-   SEGM_2AFC_trigram [speech segmentation - SEGM 2AFC trigram]
-   SEGM_2AFC_bigram [speech segmentation - SEGM 2AFC bigram]

3.  **SEGM production**:

-   SEGM_production [speech segmentation - SEGM production]

4.  **AGL**:

-   AGL_2AFC_sentence [artificial grammar learning - AGL 2AFC sentence ]
-   AGL_production [artificial grammar learning - AGL production]

5.  **AGL phrase**:

-   AGL_2AFC_phrase [artificial grammar learning - AGL 2AFC phrase]

## 2.3. Latent factors in core cognitive domain

**Kaiser-Meyer-Olkin test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
    simon_RT,
    simon_accuracy
  ) %>% 
KMO()
```

Indices with unsatisfactory fit for EFA (MSA \< 0.6) were excluded: \*
simon_accuracy [simon - accuracy] \* stroop_accuracy [stroop - accuracy]

Then, KMO test was repeated.

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    simon_RT,
  ) %>% 
KMO()
```

**Bartlett's test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    simon_RT,
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    simon_RT,
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

**Exploratory factor analysis**:

```{r echo = FALSE, warning = FALSE}
COG_EFA = df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    simon_RT,
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 4,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r echo = FALSE, warning = FALSE}
summary(COG_EFA)
```

```{r echo = FALSE, warning = FALSE}
COG_EFA$loadings
tidy(COG_EFA$uniquenesses)
```

------------------------------------------------------------------------

For further analysis, we performed hierarchical variable clustering on
the core cognitive variables as well.

**Hierarchical cluster analysis**:

```{r echo = FALSE, warning = FALSE}
tree_01 = data.frame(df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    simon_RT,
  )) %>% 
hclustvar()
```

```{r echo = FALSE, warning = FALSE}
plot(tree_01)
```

```{r echo = FALSE, warning = FALSE}
stab = stability(tree_01, B=100)
```

Since the Rand criterion shows that there is two local maxima (at 2 and
4) regarding the number of clusters, we chose the bigger number for more
detailed analysis.

**K-means cluster analysis**:

```{r echo = FALSE, warning = FALSE}
km = data.frame(df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    simon_RT,
  )) %>% 
kmeansvar(init = 4, nstart = 1000)
```

```{r echo = FALSE, warning = FALSE}
summary(km)
```

------------------------------------------------------------------------

In conclusion, we have 4 clusters of cognitive variables:

1.  **perceptual speed**:

-   perceptual_speed_visual_RT [perceptual speed - visual reaction time]
-   perceptual_speed_auditory_RT [perceptual speed - auditory reaction
    time]
-   perceptual_speed_visual_decision [perceptual speed - visual decision
    time]

2.  **digit span**:

-   digit_span_forward [digit span - forward digit span]
-   digit_span_backward [digit span - backward digit span]

3.  **nback test**:

-   nback_1_back_dprime [n-back - 1-back d-prime]
-   nback_2_back_dprime [n-back - 2-back d-prime]
-   nback_3_back_dprime [n-back - 3-back d-prime]

4.  **cognitive control**:

-   stroop_RT [stroop - RT]
-   simon_RT [simon - RT]

As the following step, we tested whether these factors shows an
appropriate fit using CFA.

**Confirmatory factor analysis**:

```{r echo = FALSE, warning = FALSE}
model_COG =
"
  COG_PS =~ perceptual_speed_visual_RT + perceptual_speed_visual_decision + perceptual_speed_auditory_RT
  COG_DS =~ digit_span_forward + digit_span_backward
  COG_nback =~ nback_1_back_dprime + nback_2_back_dprime + nback_3_back_dprime
  COG_RT =~ stroop_RT + simon_RT
"
```

**Model fits for CFA**:

```{r echo = FALSE, warning = FALSE}
model_COG_fit = cfa(model_COG, df, missing = "ML", estimator = "MLR")
summary(model_COG_fit, standardized = TRUE, fit.measures = TRUE)
```

In three cases the unexplained variance is very high (\>0.65). 1. First,
we excluded the *nback_1_dprime* variable, since it does not carry more
information compared to *nback_2_dprime* and *nback_3_dprime* variables
both theoretically and measurement-wise. 2. Secondly, we split the
*COG_RT factor* into two, since the *stroop* and *simon* tasks have
potentially additional exploratory power in the model. 3. Thirdly, we
split the *COG_PS factor* into two. (For detail see next section.)

------------------------------------------------------------------------

To split the COG_PS into two factor, we performed two CFA analyses with
nested models (*model_PS_01* and *model_PS_02*).

**First model / Model_PS_01**:

```{r echo = FALSE, warning = FALSE}
model_PS_01 =
"
  PS_01 =~ perceptual_speed_visual_RT + perceptual_speed_visual_decision
  PS_02 =~ perceptual_speed_auditory_RT
  COG_DS =~ digit_span_forward + digit_span_backward
  COG_nback =~ nback_2_back_dprime + nback_3_back_dprime
  COG_stroop =~ stroop_RT 
  COG_simon =~ simon_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_PS_01_fit = cfa(model_PS_01, df, missing = "ML", estimator = "MLR")
summary(model_PS_01_fit, standardized = TRUE, fit.measures = TRUE)
```

**Second model / Model_PS_02**:

```{r echo = FALSE, warning = FALSE}
model_PS_02 =
"
  PS_01 =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
  PS_02 =~ perceptual_speed_visual_decision
  COG_DS =~ digit_span_forward + digit_span_backward
  COG_nback =~ nback_2_back_dprime + nback_3_back_dprime
  COG_stroop =~ stroop_RT 
  COG_simon =~ simon_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_PS_02_fit = cfa(model_PS_02, df, missing = "ML", estimator = "MLR")
summary(model_PS_02_fit, standardized = TRUE, fit.measures = TRUE)
```

Based on the two models, we concluded that *model_PS_02* indicates a
better model considering four fit indices (TLI, CFI, SRMR, RMSEA) and
the BIC. So for further analyses, we used the
*perceptual_speed_visual_decision [perceptual speed - visual decision
time]* index as a single-variable latent factor retaining the other two
variables as the *PS factor*.

------------------------------------------------------------------------

The final grouping of the variables is the following structure:

1.  **perceptual speed**:

-   perceptual_speed_visual_RT [perceptual speed - visual reaction time]
-   perceptual_speed_auditory_RT [perceptual speed - auditory reaction
    time]

2.  **visual decision speed**:\

-   perceptual_speed_visual_decision [perceptual speed - visual decision
    time]

3.  **digit span**:

-   digit_span_forward [digit span - forward digit span]
-   digit_span_backward [digit span - backward digit span]

4.  **nback test**:

-   nback_2_back_dprime [n-back - 2-back d-prime]
-   nback_3_back_dprime [n-back - 3-back d-prime]

5.  **Stroop task**:

-   stroop_RT [stroop - RT]

6.  **Simon task**:

-   simon_RT [simon - RT]

## 2.4. Latent factors in language domain

**Kaiser-Meyer-Olkin test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    predictive_sent_proc_accuracy,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
KMO()
```

We excluded the *predictive_sent_proc_accuracy [predictive sentence
processing - accuracy]* index which showed unacceptable KMO values (\<
0.5).

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
KMO()
```

All indices are now in the acceptable range (\> 0.6), and the
*predictive_sent_proc_RT [predictive sentence processing - RT]* is also
near the boundary.

**Parallel analysis**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates 2 factors emerging.

**Exploratory factor analysis**:

```{r echo = FALSE, warning = FALSE}
LANG_EFA = df %>% 
  select(
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 2,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r echo = FALSE, warning = FALSE}
summary(LANG_EFA)
```

```{r echo = FALSE, warning = FALSE}
LANG_EFA$loadings
tidy(LANG_EFA$uniquenesses)
```

Since the 2-factor model showed a complex structure, we proceeded with
hierarchical cluster analysis to reveal potential single-variable
factors.

**Hierarchical cluster analysis**:

```{r echo = FALSE, warning = FALSE}
tree_01 = data.frame(df %>% 
  select(
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  )) %>% 
hclustvar()
```

```{r echo = FALSE, warning = FALSE}
plot(tree_01)
```

```{r echo = FALSE, warning = FALSE}
stab = stability(tree_01, B=100)
```

**K-means cluster analysis**:

```{r echo = FALSE, warning = FALSE}
km = data.frame(df %>% 
  select(
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  )) %>% 
kmeansvar(init = 5, nstart = 1000)
```

```{r echo = FALSE, warning = FALSE}
summary(km)
```

As a next step, we tested whether these factors show appropriate fit
using CFA.

**Confirmatory factor analysis**:

```{r echo = FALSE, warning = FALSE}
model_LANG =
"
  LANG_MENYET =~ grammatical_sensitivity
  LANG_OMR =~ one_minute_reading
  LANG_pred =~ predictive_sent_proc_RT
  LANG_space =~ selfpaced_gardenpath_target + selfpaced_violation_processing_target
  LANG_TROG =~ pragmatic_comprehension
"
```

**Fit indices for CFA**:

```{r echo = FALSE, warning = FALSE}
model_LANG_fit = cfa(model_LANG, df, missing = "ML", estimator = "MLR")
summary(model_LANG_fit, standardized = TRUE, fit.measures = TRUE)
```

In conclusion, we used 5 clusters of language related variables to
identify latent factors which serve as outcome in the SEM models.

The final structure is:

1.  **grammatical sensitivity**

-   grammatical_sensitivity [grammatical sensitivity]

2.  **one-minute reading**

-   one_minute_reading [one-minute reading]

3.  **predictive sentence processing**

-   predictive_sent_proc_RT [predictive sentence processing - RT]

4.  **self-paced reading**

-   selfpaced_gardenpath_target [self-paced reading - garden path
    sentence processing]
-   selfpaced_violation_processing_target [self-paced reading - sentence
    violation processing]

5.  **pragmatic comprehension**

-   pragmatic_comprehension [KOBAK - pragmatic comprehension]

## 2.5. Summary of the latent factors

**Table 2**

*Latent factors and constituent indices*

| domain | index                             | latent factor                |
|-------------------|----------------------------|--------------------------|
| SL     | SEGM RT TRN3--RND4                | SEGM RT                        |
| SL     | SEGM RT RND4--REC5                | SEGM RT                        |
| SL     | SEGM 2AFC bigram                  | SEGM 2AFC                      |
| SL     | SEGM 2AFC trigram                 | SEGM 2AFC                      |
| SL     | SEGM production                   | SEGM production                |
| SL     | AGL 2AFC sentence                 | AGL                            |
| SL     | AGL 2AFC production               | AGL                            |
| SL     | AGL phrase                        | AGL phrase                     |
| LANG   | grammatical sensitivity           | grammatical sensitivity        |
| LANG   | pragmatic comprehension           | pragmatic comprehension        |
| LANG   | garden path sentence processing   | self-paced reading             |
| LANG   | sentence violation processing     | self-paced reading             |
| LANG   | predictive sentence processing RT | predictive sentence processing |
| LANG   | one minute reading                | one minute reading             |
| COG    | visual reaction time              | perceptual speed               |
| COG    | auditory reaction time            | perceptual speed               |
| COG    | visual decision time              | visual decision speed          |
| COG    | forward digit span                | digit span                     |
| COG    | backward digit span               | digit span                     |
| COG    | 2-back d-prime                    | nback                          |
| COG    | 3-back d-prime                    | nback                          |
| COG    | Stroop RT                         | Stroop                         |
| COG    | Simon RT                          | Simon                          |

# 3. Mediation models (Structural equation modelling)

After the identification of latent factors, we proceeded with assembling
the models for mediation analysis. In each model, there was one
statistical learning variable, all the core cognitive variables, and the
language variables. We specified the models with lavaan syntax. CFA
estimator was Robust Maximum Likelihood method. Missing values were
imputed using Maximum Likelihood method. An optimal model fit was
ascertained based on Comparative Fit Index (CFI) and Tucker-Lewis Index
(TLI) values ranging between 0.90--0.95, and an acceptable model fit was
indicated by a Root Mean Squared Error of Approximation (RMSEA) index
below 0.05 and Standardised Root Mean Square Residual (SRMR) index below
0.08. Inflated indices were expected due to single-item factors.

## 3.1. Calculating error variances from reliability indices

```{r echo = FALSE, warning = FALSE}
# Error variances
stroop_e = (1-0.829)*var(df$stroop_RT, na.rm = TRUE)
simon_e = (1-0.724)*var(df$simon_RT, na.rm = TRUE)
visdec_e = (1-0.955)*var(df$perceptual_speed_visual_decision, na.rm = TRUE)
menyet_e = (1-0.661)*var(df$grammatical_sensitivity, na.rm = TRUE)
pred_e = (1-0.866)*var(df$predictive_sent_proc_RT, na.rm = TRUE)
trog_e = (1-0.666)*var(df$pragmatic_comprehension, na.rm = TRUE)
```

## 3.2. SEGM_RT model

```{r echo = FALSE, warning = FALSE}
model_RT = 
"
# statistical learning
SL =~ 1*SEGM_RT_TRN3_RND4 + 1*SEGM_RT_RND4_REC5

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
simon =~ simon_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_gardenpath_target + selfpaced_violation_processing_target
OMR =~ one_minute_reading
TROG =~ pragmatic_comprehension

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
simon_RT ~~ 0.274*simon_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
pragmatic_comprehension ~~ 0.326*pragmatic_comprehension
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_RT = sem(model_RT, df, missing = "ML", estimator = "MLR")
summary(model_fit_RT, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.3. SEGM_2_AFC model

```{r echo = FALSE, warning = FALSE}
model_SEGM_tribi = 
"
# statistical learning
SL =~ SEGM_2AFC_trigram + SEGM_2AFC_bigram

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
simon =~ simon_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_gardenpath_target + selfpaced_violation_processing_target
OMR =~ one_minute_reading
TROG =~ pragmatic_comprehension

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
simon_RT ~~ 0.274*simon_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
pragmatic_comprehension ~~ 0.326*pragmatic_comprehension
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_SEGM_tribi = sem(model_SEGM_tribi, df, missing = "ML", estimator = "MLR")
summary(model_fit_SEGM_tribi, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.4. SEGM_production model

```{r echo = FALSE, warning = FALSE}
model_SEGM_prod = 
"
# statistical learning
SL =~ SEGM_production

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
simon =~ simon_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_gardenpath_target + selfpaced_violation_processing_target
OMR =~ one_minute_reading
TROG =~ pragmatic_comprehension

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
simon_RT ~~ 0.274*simon_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
pragmatic_comprehension ~~ 0.326*pragmatic_comprehension
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_SEGM_prod = sem(model_SEGM_prod, df, missing = "ML", estimator = "MLR")
summary(model_fit_SEGM_prod, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.5. AGL_model

```{r echo = FALSE, warning = FALSE}
model_AGL = 
"
# statistical learning
SL =~ AGL_2AFC_sentence + AGL_production

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
simon =~ simon_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_gardenpath_target + selfpaced_violation_processing_target
OMR =~ one_minute_reading
TROG =~ pragmatic_comprehension

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
simon_RT ~~ 0.274*simon_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
pragmatic_comprehension ~~ 0.326*pragmatic_comprehension
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_AGL = sem(model_AGL, df, missing = "ML", estimator = "MLR")
summary(model_fit_AGL, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.6. AGL_phrase model

```{r echo = FALSE, warning = FALSE}
model_phr = 
"
# statistical learning
SL =~ AGL_2AFC_phrase

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
simon =~ simon_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_gardenpath_target + selfpaced_violation_processing_target
OMR =~ one_minute_reading
TROG =~ pragmatic_comprehension

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
simon_RT ~~ 0.274*simon_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
pragmatic_comprehension ~~ 0.326*pragmatic_comprehension
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_phr = sem(model_phr, df, missing = "ML", estimator = "MLR")
summary(model_fit_phr, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.7. Unified model

We also tested a unified model With all the variables for calculating
total effect sizes.

```{r echo = FALSE, warning = FALSE}
model_full =
"
# statistical learning
SL_RT =~ SEGM_RT_TRN3_RND4 + SEGM_RT_RND4_REC5
SL_AGL =~ AGL_2AFC_sentence + AGL_production
SL_SEGM_prod =~ SEGM_production
SL_SEGM_tribi =~ SEGM_2AFC_trigram + SEGM_2AFC_bigram
SL_phr =~ AGL_2AFC_phrase

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
simon =~ simon_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_gardenpath_target + selfpaced_violation_processing_target
OMR =~ one_minute_reading
TROG =~ pragmatic_comprehension

# dependent regression
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop + c11*SL_RT + c12*SL_AGL + c13*SL_SEGM_prod + c14*SL_SEGM_tribi + c15*SL_phr
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop + c21*SL_RT + c22*SL_AGL + c23*SL_SEGM_prod + c24*SL_SEGM_tribi + c25*SL_phr
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop + c31*SL_RT + c32*SL_AGL + c33*SL_SEGM_prod + c34*SL_SEGM_tribi + c35*SL_phr
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop + c41*SL_RT + c42*SL_AGL + c43*SL_SEGM_prod + c44*SL_SEGM_tribi + c45*SL_phr
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop + c51*SL_RT + c52*SL_AGL + c53*SL_SEGM_prod + c54*SL_SEGM_tribi + c55*SL_phr

# mediator regression
DS ~ a11*SL_RT + a12*SL_AGL + a13*SL_SEGM_prod + a14*SL_SEGM_tribi + a15*SL_phr
simon ~ a21*SL_RT + a22*SL_AGL + a23*SL_SEGM_prod + a24*SL_SEGM_tribi + a25*SL_phr
PS_visdec ~ a31*SL_RT + a32*SL_AGL + a33*SL_SEGM_prod + a34*SL_SEGM_tribi + a35*SL_phr
nback ~ a41*SL_RT + a42*SL_AGL + a43*SL_SEGM_prod + a44*SL_SEGM_tribi + a45*SL_phr
PS ~ a51*SL_RT + a52*SL_AGL + a53*SL_SEGM_prod + a54*SL_SEGM_tribi + a55*SL_phr
stroop ~ a61*SL_RT + a62*SL_AGL + a63*SL_SEGM_prod + a64*SL_SEGM_tribi + a65*SL_phr

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR
TROG ~~ MENYET

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y1 ~ x2
ind_x2_m1_y1 := a12*b11
ind_x2_m2_y1 := a22*b12
ind_x2_m3_y1 := a32*b13
ind_x2_m4_y1 := a42*b14
ind_x2_m5_y1 := a52*b15
ind_x2_m6_y1 := a62*b16
ind_x2_y1 := ind_x2_m1_y1 + ind_x2_m2_y1 + ind_x2_m3_y1 + ind_x2_m4_y1 + ind_x2_m5_y1 + ind_x2_m6_y1
tot_x2_y1 := ind_x2_y1 + c12

# y1 ~ x3
ind_x3_m1_y1 := a13*b11
ind_x3_m2_y1 := a23*b12
ind_x3_m3_y1 := a33*b13
ind_x3_m4_y1 := a43*b14
ind_x3_m5_y1 := a53*b15
ind_x3_m6_y1 := a63*b16
ind_x3_y1 := ind_x3_m1_y1 + ind_x3_m2_y1 + ind_x3_m3_y1 + ind_x3_m4_y1 + ind_x3_m5_y1 + ind_x3_m6_y1
tot_x3_y1 := ind_x3_y1 + c13

# y1 ~ x4
ind_x4_m1_y1 := a14*b11
ind_x4_m2_y1 := a24*b12
ind_x4_m3_y1 := a34*b13
ind_x4_m4_y1 := a44*b14
ind_x4_m5_y1 := a54*b15
ind_x4_m6_y1 := a64*b16
ind_x4_y1 := ind_x4_m1_y1 + ind_x4_m2_y1 + ind_x4_m3_y1 + ind_x4_m4_y1 + ind_x4_m5_y1 + ind_x4_m6_y1
tot_x4_y1 := ind_x4_y1 + c14

# y1 ~ x5
ind_x5_m1_y1 := a15*b11
ind_x5_m2_y1 := a25*b12
ind_x5_m3_y1 := a35*b13
ind_x5_m4_y1 := a45*b14
ind_x5_m5_y1 := a55*b15
ind_x5_m6_y1 := a65*b16
ind_x5_y1 := ind_x5_m1_y1 + ind_x5_m2_y1 + ind_x5_m3_y1 + ind_x5_m4_y1 + ind_x5_m5_y1 + ind_x5_m6_y1
tot_x5_y1 := ind_x5_y1 + c15

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y2 ~ x2
ind_x2_m1_y2 := a12*b21
ind_x2_m2_y2 := a22*b22
ind_x2_m3_y2 := a32*b23
ind_x2_m4_y2 := a42*b24
ind_x2_m5_y2 := a52*b25
ind_x2_m6_y2 := a62*b26
ind_x2_y2 := ind_x2_m1_y2 + ind_x2_m2_y2 + ind_x2_m3_y2 + ind_x2_m4_y2 + ind_x2_m5_y2 + ind_x2_m6_y2
tot_x2_y2 := ind_x2_y2 + c22

# y2 ~ x3
ind_x3_m1_y2 := a13*b21
ind_x3_m2_y2 := a23*b22
ind_x3_m3_y2 := a33*b23
ind_x3_m4_y2 := a43*b24
ind_x3_m5_y2 := a53*b25
ind_x3_m6_y2 := a63*b26
ind_x3_y2 := ind_x3_m1_y2 + ind_x3_m2_y2 + ind_x3_m3_y2 + ind_x3_m4_y2 + ind_x3_m5_y2 + ind_x3_m6_y2
tot_x3_y2 := ind_x3_y2 + c23

# y2 ~ x4
ind_x4_m1_y2 := a14*b21
ind_x4_m2_y2 := a24*b22
ind_x4_m3_y2 := a34*b23
ind_x4_m4_y2 := a44*b24
ind_x4_m5_y2 := a54*b25
ind_x4_m6_y2 := a64*b26
ind_x4_y2 := ind_x4_m1_y2 + ind_x4_m2_y2 + ind_x4_m3_y2 + ind_x4_m4_y2 + ind_x4_m5_y2 + ind_x4_m6_y2
tot_x4_y2 := ind_x4_y2 + c24

# y2 ~ x5
ind_x5_m1_y2 := a15*b21
ind_x5_m2_y2 := a25*b22
ind_x5_m3_y2 := a35*b23
ind_x5_m4_y2 := a45*b24
ind_x5_m5_y2 := a55*b25
ind_x5_m6_y2 := a65*b26
ind_x5_y2 := ind_x5_m1_y2 + ind_x5_m2_y2 + ind_x5_m3_y2 + ind_x5_m4_y2 + ind_x5_m5_y2 + ind_x5_m6_y2
tot_x5_y2 := ind_x5_y2 + c25

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y3 ~ x2
ind_x2_m1_y3 := a12*b31
ind_x2_m2_y3 := a22*b32
ind_x2_m3_y3 := a32*b33
ind_x2_m4_y3 := a42*b34
ind_x2_m5_y3 := a52*b35
ind_x2_m6_y3 := a62*b36
ind_x2_y3 := ind_x2_m1_y3 + ind_x2_m2_y3 + ind_x2_m3_y3 + ind_x2_m4_y3 + ind_x2_m5_y3 + ind_x2_m6_y3
tot_x2_y3 := ind_x2_y3 + c32

# y3 ~ x3
ind_x3_m1_y3 := a13*b31
ind_x3_m2_y3 := a23*b32
ind_x3_m3_y3 := a33*b33
ind_x3_m4_y3 := a43*b34
ind_x3_m5_y3 := a53*b35
ind_x3_m6_y3 := a63*b36
ind_x3_y3 := ind_x3_m1_y3 + ind_x3_m2_y3 + ind_x3_m3_y3 + ind_x3_m4_y3 + ind_x3_m5_y3 + ind_x3_m6_y3
tot_x3_y3 := ind_x3_y3 + c33

# y3 ~ x4
ind_x4_m1_y3 := a14*b31
ind_x4_m2_y3 := a24*b32
ind_x4_m3_y3 := a34*b33
ind_x4_m4_y3 := a44*b34
ind_x4_m5_y3 := a54*b35
ind_x4_m6_y3 := a64*b36
ind_x4_y3 := ind_x4_m1_y3 + ind_x4_m2_y3 + ind_x4_m3_y3 + ind_x4_m4_y3 + ind_x4_m5_y3 + ind_x4_m6_y3
tot_x4_y3 := ind_x4_y3 + c34

# y3 ~ x5
ind_x5_m1_y3 := a15*b31
ind_x5_m2_y3 := a25*b32
ind_x5_m3_y3 := a35*b33
ind_x5_m4_y3 := a45*b34
ind_x5_m5_y3 := a55*b35
ind_x5_m6_y3 := a65*b36
ind_x5_y3 := ind_x5_m1_y3 + ind_x5_m2_y3 + ind_x5_m3_y3 + ind_x5_m4_y3 + ind_x5_m5_y3 + ind_x5_m6_y3
tot_x5_y3 := ind_x5_y3 + c35

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y4 ~ x2
ind_x2_m1_y4 := a12*b41
ind_x2_m2_y4 := a22*b42
ind_x2_m3_y4 := a32*b43
ind_x2_m4_y4 := a42*b44
ind_x2_m5_y4 := a52*b45
ind_x2_m6_y4 := a62*b46
ind_x2_y4 := ind_x2_m1_y4 + ind_x2_m2_y4 + ind_x2_m3_y4 + ind_x2_m4_y4 + ind_x2_m5_y4 + ind_x2_m6_y4
tot_x2_y4 := ind_x2_y4 + c42

# y4 ~ x3
ind_x3_m1_y4 := a13*b41
ind_x3_m2_y4 := a23*b42
ind_x3_m3_y4 := a33*b43
ind_x3_m4_y4 := a43*b44
ind_x3_m5_y4 := a53*b45
ind_x3_m6_y4 := a63*b46
ind_x3_y4 := ind_x3_m1_y4 + ind_x3_m2_y4 + ind_x3_m3_y4 + ind_x3_m4_y4 + ind_x3_m5_y4 + ind_x3_m6_y4
tot_x3_y4 := ind_x3_y4 + c43

# y4 ~ x4
ind_x4_m1_y4 := a14*b41
ind_x4_m2_y4 := a24*b42
ind_x4_m3_y4 := a34*b43
ind_x4_m4_y4 := a44*b44
ind_x4_m5_y4 := a54*b45
ind_x4_m6_y4 := a64*b46
ind_x4_y4 := ind_x4_m1_y4 + ind_x4_m2_y4 + ind_x4_m3_y4 + ind_x4_m4_y4 + ind_x4_m5_y4 + ind_x4_m6_y4
tot_x4_y4 := ind_x4_y4 + c44

# y4 ~ x5
ind_x5_m1_y4 := a15*b41
ind_x5_m2_y4 := a25*b42
ind_x5_m3_y4 := a35*b43
ind_x5_m4_y4 := a45*b44
ind_x5_m5_y4 := a55*b45
ind_x5_m6_y4 := a65*b46
ind_x5_y4 := ind_x5_m1_y4 + ind_x5_m2_y4 + ind_x5_m3_y4 + ind_x5_m4_y4 + ind_x5_m5_y4 + ind_x5_m6_y4
tot_x5_y4 := ind_x5_y4 + c45

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# y5 ~ x2
ind_x2_m1_y5 := a12*b51
ind_x2_m2_y5 := a22*b52
ind_x2_m3_y5 := a32*b53
ind_x2_m4_y5 := a42*b54
ind_x2_m5_y5 := a52*b55
ind_x2_m6_y5 := a62*b56
ind_x2_y5 := ind_x2_m1_y5 + ind_x2_m2_y5 + ind_x2_m3_y5 + ind_x2_m4_y5 + ind_x2_m5_y5 + ind_x2_m6_y5
tot_x2_y5 := ind_x2_y5 + c52

# y5 ~ x3
ind_x3_m1_y5 := a13*b51
ind_x3_m2_y5 := a23*b52
ind_x3_m3_y5 := a33*b53
ind_x3_m4_y5 := a43*b54
ind_x3_m5_y5 := a53*b55
ind_x3_m6_y5 := a63*b56
ind_x3_y5 := ind_x3_m1_y5 + ind_x3_m2_y5 + ind_x3_m3_y5 + ind_x3_m4_y5 + ind_x3_m5_y5 + ind_x3_m6_y5
tot_x3_y5 := ind_x3_y5 + c53

# y5 ~ x4
ind_x4_m1_y5 := a14*b51
ind_x4_m2_y5 := a24*b52
ind_x4_m3_y5 := a34*b53
ind_x4_m4_y5 := a44*b54
ind_x4_m5_y5 := a54*b55
ind_x4_m6_y5 := a64*b56
ind_x4_y5 := ind_x4_m1_y5 + ind_x4_m2_y5 + ind_x4_m3_y5 + ind_x4_m4_y5 + ind_x4_m5_y5 + ind_x4_m6_y5
tot_x4_y5 := ind_x4_y5 + c54

# y5 ~ x5
ind_x5_m1_y5 := a15*b51
ind_x5_m2_y5 := a25*b52
ind_x5_m3_y5 := a35*b53
ind_x5_m4_y5 := a45*b54
ind_x5_m5_y5 := a55*b55
ind_x5_m6_y5 := a65*b56
ind_x5_y5 := ind_x5_m1_y5 + ind_x5_m2_y5 + ind_x5_m3_y5 + ind_x5_m4_y5 + ind_x5_m5_y5 + ind_x5_m6_y5
tot_x5_y5 := ind_x5_y5 + c55

# y ~ x(all)

ind_y1 := ind_x1_y1 + ind_x2_y1 + ind_x3_y1 + ind_x4_y1 + ind_x5_y1
tot_y1 := tot_x1_y1 + tot_x2_y1 + tot_x3_y1 + tot_x4_y1 + tot_x5_y1

ind_y2 := ind_x1_y2 + ind_x2_y2 + ind_x3_y2 + ind_x4_y2 + ind_x5_y2
tot_y2 := tot_x1_y2 + tot_x2_y2 + tot_x3_y2 + tot_x4_y2 + tot_x5_y2

ind_y3 := ind_x1_y3 + ind_x2_y3 + ind_x3_y3 + ind_x4_y3 + ind_x5_y3
tot_y3 := tot_x1_y3 + tot_x2_y3 + tot_x3_y3 + tot_x4_y3 + tot_x5_y3

ind_y4 := ind_x1_y4 + ind_x2_y4 + ind_x3_y4 + ind_x4_y4 + ind_x5_y4
tot_y4 := tot_x1_y4 + tot_x2_y4 + tot_x3_y4 + tot_x4_y4 + tot_x5_y4

ind_y5 := ind_x1_y5 + ind_x2_y5 + ind_x3_y5 + ind_x4_y5 + ind_x5_y5
tot_y5 := tot_x1_y5 + tot_x2_y5 + tot_x3_y5 + tot_x4_y5 + tot_x5_y5

dir_y1 := tot_y1 - ind_y1
dir_y2 := tot_y2 - ind_y2
dir_y3 := tot_y3 - ind_y3
dir_y4 := tot_y4 - ind_y4
dir_y5 := tot_y5 - ind_y5

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
simon_RT ~~ 0.274*simon_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_full = sem(model_full, df, missing = "ML", estimator = "MLR")
summary(model_fit_full, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.8. Summary of mediation models

**Table 3**

*Fit indices of mediation models*

| model           | CFI  | R-CFI | TLI  | R-TLI | R-RMSEA | SRMR |
|-----------------|------|-------|------|-------|---------|------|
| SEGM RT         | .983 | .976  | .961 | .946  | .040    | .028 |
| SEGM 2AFC       | .992 | .999  | .983 | .999  | .006    | .024 |
| SEGM production | .991 | .996  | .975 | .989  | .018    | .022 |
| AGL             | .989 | .995  | .975 | .988  | .018    | .025 |
| AGL phrase      | .988 | .992  | .969 | .979  | .024    | .024 |
| SL overall      | .988 | .985  | .974 | .970  | .025    | .032 |

*Note.* CFI: = Comparative Fit Index, R-CFI: = Robust Comparative Fit
Index, TLI: = Tucker-Lewis Index, R-TLI: = Robust Tucker-Lewis Index,
R-RMSEA: = Robust Root Mean Square Error of Approximation, SRMR: =
Standardised Root Mean Square Residual.

**Table 4**

*Reliability values and average variance extracted (AVE) for latent
variables*

```{r echo = FALSE, warning = FALSE}
reliability(model_fit_RT)
reliability(model_fit_SEGM_tribi)
reliability(model_fit_SEGM_prod)
reliability(model_fit_AGL)
reliability(model_fit_phr)
reliability(model_fit_full)
```

**Table 5**

*Mediation models*

| predictor       | outcome                        | direct effect | indirect effect | total effect |
|---------------|---------------|---------------|---------------|---------------|
| SEGM RT         | grammatical sensitivity        | -.115         | .114\*          | -.001        |
|                 | predictive sentence processing | -.071         | .068\*          | -.003        |
|                 | self-paced reading             | .046          | .128\*\*        | .174         |
|                 | one-minute reading             | .065          | .094\*          | .159\*       |
|                 | pragmatic comprehension        | .130          | .101\*          | .231\*\*     |
| SEGM 2AFC       | grammatical sensitivity        | .394\*        | .099\*          | .493\*\*     |
|                 | predictive sentence processing | .099          | .074            | .174         |
|                 | self-paced reading             | .033          | .166\*\*        | .198         |
|                 | one-minute reading             | -.151         | .157\*\*        | .005         |
|                 | pragmatic comprehension        | .144          | .118\*          | .262\*       |
| SEGM production | grammatical sensitivity        | .304\*\*      | .145\*\*        | .450\*\*\*   |
|                 | predictive sentence processing | -.047         | .136\*\*        | .088         |
|                 | self-paced reading             | -.062         | .193\*\*        | .132         |
|                 | one-minute reading             | .114          | .111\*          | .225\*\*\*   |
|                 | pragmatic comprehension        | .258\*\*      | .138\*\*        | .396\*\*\*   |
| AGL             | grammatical sensitivity        | .143          | .233\*          | .376\*\*\*   |
|                 | predictive sentence processing | .156          | .114            | .269\*\*     |
|                 | self-paced reading             | .077          | .213\*          | .290\*       |
|                 | one-minute reading             | .259          | .072            | .331\*\*\*   |
|                 | pragmatic comprehension        | .446\*        | .069            | .515\*\*\*   |
| AGL phrase      | grammatical sensitivity        | -.018         | .125\*\*        | .106         |
|                 | predictive sentence processing | .031          | .062\*          | .093         |
|                 | self-paced reading             | .128          | .071            | .199         |
|                 | one-minute reading             | -.072         | .073\*          | .002         |
|                 | pragmatic comprehension        | .109          | .081\*          | .190\*       |
| SL overall      | grammatical sensitivity        | .336          | .215            | .551\*\*\*   |
|                 | predictive sentence processing | .079          | .174            | .253\*\*     |
|                 | self-paced reading             | .152          | .340\*          | .491\*\*     |
|                 | one-minute reading             | .191          | .117            | .308\*\*     |
|                 | pragmatic comprehension        | .625          | -.009           | .617\*\*\*   |

*Note.* \*: p \< .05, \*\*: p \< .01, \*\*\*: p \< .001

# 4. Complementary models for testing reversed pathways

## 4.1. Reversed independent - mediator model

```{r echo = FALSE, warning = FALSE}
model_a_reverse =
"
# statistical learning
SL_RT =~ SEGM_RT_TRN3_RND4 + SEGM_RT_RND4_REC5
SL_AGL =~ AGL_2AFC_sentence + AGL_production
SL_SEGM_prod =~ SEGM_production
SL_SEGM_tribi =~ SEGM_2AFC_trigram + SEGM_2AFC_bigram
SL_phr =~ AGL_2AFC_phrase

# cognitive
COG =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT + digit_span_forward + digit_span_backward + nback_2_back_dprime + nback_3_back_dprime + stroop_RT + simon_RT + perceptual_speed_visual_decision

# reverse a path regression (cog -> statistical learning)
SL_RT ~ a11*COG
SL_AGL ~ a12*COG
SL_SEGM_prod ~ a13*COG
SL_SEGM_tribi ~ a14*COG
SL_phr ~ a15*COG

# dependent residual covariance
SL_RT ~~ SL_AGL
SL_RT ~~ SL_SEGM_prod
SL_AGL ~~ SL_SEGM_prod
SL_RT ~~ SL_SEGM_tribi
SL_AGL ~~ SL_SEGM_tribi
SL_SEGM_prod ~~ SL_SEGM_tribi
SL_phr ~~ SL_SEGM_prod
SL_phr ~~ SL_AGL
SL_phr ~~ SL_SEGM_tribi
SL_phr ~~ SL_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_a_reverse_fit = sem(model_a_reverse, df, missing = "ML", estimator = "MLR")
summary(model_a_reverse_fit, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 4.2. Reversed mediator - dependent model

```{r echo = FALSE, warning = FALSE}
model_b_reverse =
"
# cognitive
COG =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT + digit_span_forward + digit_span_backward + nback_2_back_dprime + nback_3_back_dprime + stroop_RT + simon_RT + perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_gardenpath_target + selfpaced_violation_processing_target
OMR =~ one_minute_reading
TROG =~ pragmatic_comprehension

# reverse b path regression (cog -> lang)
MENYET ~ b11*COG
pred ~ b12*COG
space ~ b13*COG
OMR ~ b14*COG
TROG ~ b15*COG

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR
"
```

```{r echo = FALSE, warning = FALSE}
model_b_reverse_fit = sem(model_b_reverse, df, missing = "ML", estimator = "MLR")
summary(model_b_reverse_fit, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```
