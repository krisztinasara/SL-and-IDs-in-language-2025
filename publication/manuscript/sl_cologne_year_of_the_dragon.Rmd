|                                                        |
|--------------------------------------------------------|
| itle: "Individual differences in statistical learning" |
| utput: html_notebook                                   |

# Preparation

## R packages:

```{r}
library(tidyverse)
library(broom)
library(psych)
library(EFAtools)
library(lavaan)
library(ClustOfVar)
library(semTools)
library(semPlot)
library(tidySEM)
```

## Data:

```{r}
#df = read.csv("C:/Users/Kriszti/GitHub/lendulet_language_SL/SEM/dataframes/filtered_SEM_df.csv")
df = read.csv("~/GitHub/lendulet_language_SL/SEM/dataframes/filtered_SEM_df.csv")
df = df |>
  mutate(
    PROC_SPEED_vis_RT_med = -(PROC_SPEED_vis_RT_med),
    PROC_SPEED_ac_RT_med = -(PROC_SPEED_ac_RT_med),
    PROC_SPEED_vis_dec_RT_med = -(PROC_SPEED_vis_dec_RT_med),
    PROC_SPEED_ac_dec_RT_med = -(PROC_SPEED_ac_dec_RT_med),
    simon_RT_score = -(simon_RT_score),
    simon_ACC_score = -(simon_ACC_score),
    stroop_RT_score = -(stroop_RT_score),
    stroop_ACC_score = -(stroop_ACC_score),
    selfpaced_target_RT_diff_GP = -(selfpaced_target_RT_diff_GP),
    selfpaced_plus1_RT_diff_mellekmondat = -(selfpaced_plus1_RT_diff_mellekmondat),
    selfpaced_target_RT_diff_sertes = -(selfpaced_target_RT_diff_sertes)
  )
```

# 1. Selecting reliable indices

Our initial dataset comprised 12 tasks with 45 indices (Table 1). Each index has a minimum overlap of 100 with another index. For each index, two criteria were considered: (i) **unidimensionality** and (ii) **internal consistency**, indicative of its reliability. Indices failing to meet either criterion were dropped. Indices that couldn't be evaluated were retained under the assumption that they might contain valuable information for the overall model.

Unidimensionality was assessed using Exploratory Factor Analysis (EFA) on individual items within the task index. Internal consistency was evaluated using the split-half method or omega coefficient where applicable, with a minimum reliability level set at 0.5 as the inclusion criterion.

The *trog_gramm* index was excluded due to ceiling effects. For *grammatical sensitivity* and *pragmatic comprehension* indices, the item composition was modified to meet the criteria. Since these two indices consist of qualitatively different items, to prevent overfitting, we divided the sample into two halves. One half was used to select well-fitting items in the modified index, and the other half was employed to evaluate the new structure.

Consequently, 13 indices were dropped, leaving 32 indices introduced to the EFA models.

**Table 1**

*Reliability of indices of the original dataset*

| group | task                     | index                  | reliability type | reliability value | unidimensionality | suitable |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| COG   | processing speed               | visual reaction time   | split-half\*     | 0.957             | n.a.              | yes      |
| COG   | processing speed              | auditory reaction time | split-half\*     | 0.956             | n.a.              | yes      |
| COG   | processing speed              | visual decision score  | split-half\*     | 0.179             | n.a.              | no       |
| COG   | processing speed               | visual decision time   | split-half\*     | 0.952             | n.a.              | yes      |
| COG   | simon                    | simon RT               | split-half\*     | 0.643             | n.a.              | yes      |
| COG   | simon                    | simon accuracy              | split-half\*     | 0.546             | n.a.              | yes      |
| COG   | stroop                   | stroop RT               | split-half\*     | 0.768             | n.a.              | yes      |
| COG   | stroop                   | stroop accuracy              | split-half\*     | 0.876             | n.a.              | yes      |
| COG   | digit span                | forward digit span           | n.a.             | n.a.              | n.a.              | yes      |
| COG   | digit span                | backward digit span          | n.a.             | n.a.              | n.a.              | yes      |
| COG   | n-back                   | 1-back d-prime         | split-half\*     | 0.807             | n.a.              | yes      |
| COG   | n-back                   | 2-back d-prime         | split-half\*     | 0.763             | n.a.              | yes      |
| COG   | n-back                   | 3-back d-prime         | split-half\*     | 0.691             | n.a.              | yes      |
| SL    | speech segmentation | SEGM RT training     | split-half\*     | 0.646             | n.a.              | yes      |
| SL    | speech segmentation | SEGM RT TRN3-RND4      | split-half\*     | 0.768             | n.a.              | yes      |
| SL    | speech segmentation | SEGM RT RND4-REC5      | split-half\*     | 0.778             | n.a.              | yes      |
| SL    | speech segmentation | SEGM ACC training           | split-half\*     | 0.855             | n.a.              | yes      |
| SL    | speech segmentation | SEGM ACC TRN3-RND4            | split-half\*     | 0.850             | n.a.              | yes      |
| SL    | speech segmentation | SEGM ACC RND4-REC5            | split-half\*     | 0.761             | n.a.              | yes      |
| SL    | speech segmentation | SEGM 2AFC bigram            | n.a.             | n.a.              | n.a.              | yes      |
| SL    | speech segmentation | SEGM 2AFC trigram           | n.a.             | n.a.              | n.a.              | yes      |
| SL    | speech segmentation | SEGM production             | n.a.             | n.a.              | n.a.              | yes      |
| SL    | artificial grammar learning             | AGL RT training     | split-half\*     | 0.222             | n.a.              | no       |
| SL    | artificial grammar learning              | AGL RT TRN3-RND4      | split-half\*     | 0.270             | n.a.              | no       |
| SL    | artificial grammar learning              | AGL RT RND4-REC5      | split-half\*     | 0.247             | n.a.              | no       |
| SL    | artificial grammar learning              | AGL ACC training           | split-half\*     | 0.634             | n.a.              | yes      |
| SL    | artificial grammar learning              | AGL ACC TRN3-RND4            | split-half\*     | 0.380             | n.a.              | no       |
| SL    | artificial grammar learning              | AGL ACC RND4-REC5            | split-half\*     | 0.390             | n.a.              | no       |
| SL    | artificial grammar learning              | AGL 2AFC sentence          | n.a.             | n.a.              | n.a.              | yes      |
| SL    | artificial grammar learning              | AGL 2AFC phrase    | n.a.             | n.a.              | n.a.              | yes      |
| SL    | artificial grammar learning              | AGL production             | n.a.             | n.a.              | n.a.              | yes      |
| LANG  | predictive sentence processing               | predictive sentence processing RT              | split-half\*     | 0.853             | n.a.              | yes      |
| LANG  | predictive sentence processing               | predictive sentence processing accuracy              | split-half\*     | 0.585             | n.a.              | yes      |
| LANG  | KOBAK                     | grammatical comprehension                  | omega            | 0.672             | 1                 | no       |
| LANG  | KOBAK                     | pragmatic comprehension                  | omega            | 0.630             | 1                 | yes      |
| LANG  | grammatical sensitivity                   | grammatical sensitivity                       | omega            | 0.676             | 1                 | yes      |
| LANG  | self-paced reading                | garden path - target              | omega            | 0.506             | 1                 | yes      |
| LANG  | self-paced reading                | garden path - plus one              | omega            | -0.060            | 0                 | no       |
| LANG  | self-paced reading                | past participle garden path - target            | omega            | 0.270             | 0                 | no       |
| LANG  | self-paced reading                | past participle garden path - plus one            | omega            | 0.140             | 0                 | no       |
| LANG  | self-paced reading                | object relative clause processing - target    | omega            | -0.030            | 1                 | no       |
| LANG  | self-paced reading                | object relative clause processing - plus one     | omega            | 0.576             | 0                 | no       |
| LANG  | self-paced reading                | sentence violation processing - target          | omega            | 0.681             | 1                 | yes      |
| LANG  | self-paced reading                | sentence violation processing - plus one           | omega            | 0.391             | 0                 | no       |
| LANG  | one-minute reading                     | one-minute reading              | n.a.             | n.a.              | n.a.              | yes      |

*Note.* \*: Reported split-half reliabilities are the mean of 100 iterations, Spearman-Brown corrected at each iteration.

# 2. Latent variable identification

## 2.1. Summary for the process of latent variable identification

(See details below)

To identify latent variables, we initially categorized the measurement indices into three designated domains: the *statistical learning* (SL) domain, the *general cognitive* (COG) domain, and the *language* (LANG) domain (see Table 1).

In the next step, exploratory factor analyses (EFAs) were conducted within each domain, following these procedures:

1.  **KMO test**: Using the Kaiser-Meyer-Olkin test, we assessed the measures of sampling adequacy (MSA) for the indices. Subsequently, indices with unsatisfactory fit for EFA (MSA \< 0.6) were excluded.

2.  **Bartlett's test**: Secondly, Bartlett's test of sphericity was employed, with the significance level indicating a suitable fit for EFA.

3.  **Parallel analysis**: To determine the number of factors in each domain, Horn's parallel analysis was used. This method compares eigenvalues derived from the data matrix to those from a Monte Carlo simulated matrix generated from random data of the same size.

4.  **Exploratory factor analysis**: After determining the number of factors, exploratory factor analysis was conducted using maximum likelihood as the factoring method and promax (oblique) rotation. Missing values were imputed using the mean. Based on factor loadings and uniqueness observed in each index, we refined our model.

5.  **Hierarchical cluster analysis**: To validate EFA results, hierarchical cluster analyses were performed. The stability of partitions from a hierarchy of variables was evaluated using a bootstrapping method (n=100). Based on the mean adjusted Rand criterion, we examined local maximums regarding the potential number of clusters and selected the one supported by our theoretical considerations if necessary. We then proceeded with k-means clustering of variables, resulting in the identification of latent variables.

6.  **Confirmatory factor analysis**: In the final step, we validated the structure through confirmatory factor analysis. Analyses were conducted using the Robust Maximum Likelihood method, with missing values imputed by the Maximum Likelihood method. Final models were refined if necessary.

During these steps, seven indices were removed from the initial 32 indices for not meeting the KMO test criterion, and two additional indices were dropped due to model refinement. Thus, the final dataset included 23 indices from 12 different tasks (Table 2).

## 2.2. Latent variables in statistical learning domain

**Kaiser-Meyer-Olkin test**:

```{r}
df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
    SEGM_AL_ACC_train,
    SEGM_AL_ACC_TRN3_RND4,
    SEGM_AL_ACC_RND4_REC5,
    AGL_ACC_train,
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
KMO()
```

Indices with unsatisfactory fit for EFA (MSA \< 0.5) were excluded: 
- SEGM_AL_ACC_train [speech segmentation - SEGM ACC training] 
- SEGM_AL_ACC_TRN3_RND4 [speech segmentation - SEGM ACC TRN3-RND4] 
- SEGM_AL_ACC_RND4_REC5 [speech segmentation - SEGM ACC RND4-REC5]
- AGL_ACC_train [artificial grammar learning - AGL ACC training]

Then, KMO test was repeated.

```{r}
df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
KMO()
```

The reaction time based indices showed poor fit (\< 0.6), however, we did not omit them for their possible exploratory power. Instead, we performed two separate analysis with only the reaction time based variables and the remaining ones.

### 2.2.1. RT variables

**Bartlett's test**:

```{r}
df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r}
df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates a single factor.

**Exploratory factor analysis**:

```{r}
SL_RT = df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 1,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r}
summary(SL_RT)
```

```{r}
SL_RT$loadings
tidy(SL_RT$uniquenesses)
```

### 2.2.2. Offline SL indices (non-RT indices)

**Bartlett's test**:

```{r}
df %>% 
  select(
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r}
df %>% 
  select(
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates 2 factors.

**Exploratory factor analysis**:

```{r}
SL_OFF = df %>% 
  select(
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 2,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r}
summary(SL_OFF)
```

```{r}
SL_OFF$loadings
tidy(SL_OFF$uniquenesses)
```

### 2.2.3. Final model for statistical learning variables

Since the 2-factor model showed a complex structure, we proceeded with hierarchical cluster analysis to reveal potential single-variable factors.

**Hierarchical cluster analysis**:

```{r}
tree_01 = df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
hclustvar()
```

```{r}
plot(tree_01)
```

```{r}
stab = stability(tree_01, B=100)
```

Since the Rand criterion shows that there is two local maxima (at 3 and 5) regarding the number of clusters, we selected the greater number to get a more detailed analysis.

**K-means cluster analysis**:

```{r}
km = df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
kmeansvar(init = 5, nstart = 1000)
```

```{r}
summary(km)
```

In conclusion, we used these 5 clusters of statistical learning variables to identify latent variables which later serve as predictors in the SEM models. We tested the robustness of the results with confirmatory factor analysis.

**Confirmatory factor analysis**:

```{r}
model_SL =
"
  SL_RT =~ SEGM_AL_medRT_train + SEGM_AL_medRT_TRN3_RND4 + SEGM_AL_medRT_RND4_REC5
  SL_tribi =~ SEGM_AL_2AFC_bigram + SEGM_AL_2AFC_trigram
  SL_SEGM_prod =~ SEGM_AL_SEGM_prod_data
  SL_AGL =~ AGL_2AFC_sent + AGL_prod
  SL_phr =~ AGL_2AFC_phr
"
```

**Model fits for CFA**:

```{r}
model_SL_fit = cfa(model_SL, df, missing = "ML", estimator = "MLR")
summary(model_SL_fit, standardized = TRUE, fit.measures = TRUE)
```

In one case (*SEGM_AL_2AFC_bigram [speech segmentation - SEGM 2AFC bigram]*), the unexplained variance was high (\>0.65), however we kept it to assign convergent validity to the SL_trigram variable. We further excluded the *SEGM_AL_midRT_train [speech segmentation - SEGM RT training]* index to solve the Heywood case present in this factor.

------------------------------------------------------------------------

The final grouping of the variables is:

1.  **SEGM_RT**:

-   SEGM_AL_medRT_TRN3_RND4 [speech segmentation - SEGM RT TRN3-RND4]
-   SEGM_AL_medRT_RND4_REC5 [speech segmentation - SEGM RT RND4-REC5]

2.  **SEGM_2_AFC**:

-   SEGM_AL_2AFC_trigram [speech segmentation - SEGM 2AFC trigram]
-   SEGM_AL_2AFC_bigram [speech segmentation - SEGM 2AFC bigram]

3.  **SL_SEGM_production**:

-   SEGM_AL_SEGM_prod_data [speech segmentation - SEGM production]

4.  **SL_AGL**:

-   AGL_2AFC_sent [artificial grammar learning - AGL 2AFC sentence ]
-   AGL_prod [artificial grammar learning - AGL production]

5.  **AGL_phrase**:

-   AGL_2AFC_phr [artificial grammar learning - AGL 2AFC phrase]

## 2.3. Latent variables in general cognitive domain

**Kaiser-Meyer-Olkin test**:

```{r}
df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    stroop_ACC_score,
    simon_RT_score,
    simon_ACC_score
  ) %>% 
KMO()
```

Indices with unsatisfactory fit for EFA (MSA < 0.6) were excluded: 
* simon_ACC_score [simon - accuracy]
* stroop_ACC_score [stroop - accuracy]

Then, KMO test was repeated.

```{r}
df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
KMO()
```

**Bartlett's test**:

```{r}
df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r}
df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

**Exploratory factor analysis**:

```{r}
COG_EFA = df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 4,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r}
summary(COG_EFA)
```

```{r}
COG_EFA$loadings
tidy(COG_EFA$uniquenesses)
```

------------------------------------------------------------------------

For further analysis, we performed hierarchical variable clustering on the general cognitive variables as well.

**Hierarchical cluster analysis**:

```{r}
tree_01 = df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
hclustvar()
```

```{r}
plot(tree_01)
```

```{r}
stab = stability(tree_01, B=100)
```

Since the Rand criterion shows that there is two local maxima (at 2 and 4) regarding the number of clusters, we chose the bigger number for more detailed analysis.

**K-means cluster analysis**:

```{r}
km = df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
kmeansvar(init = 4, nstart = 1000)
```

```{r}
summary(km)
```

------------------------------------------------------------------------

In conclusion, we have 4 clusters of cognitive variables:

1.  **Processing Speed**:

-   PROC_SPEED_vis_RT_med [processing speed - visual reaction time]
-   PROC_SPEED_ac_RT_med [processing speed - auditory reaction time]
-   PROC_SPEED_vis_dec_RT_med [processing speed - visual decision time]

2.  **Digit Span**:

-   digit_span_forward_span [digit span - forward digit span]
-   digit_span_backward_span [digit span - backward digit span]

3.  **NBACK test**:

-   n_back_nback_1_dprime [n-back - 1-back d-prime]
-   n_back_nback_2_dprime [n-back - 2-back d-prime]
-   n_back_nback_3_dprime [n-back - 3-back d-prime]

4.  **Reaction Time**:

-   stroop_RT_score [stroop - RT]
-   simon_RT_score [simon - RT]

As the following step, we tested whether these factors shows an appropriate fit using cfa.

**Confirmatory factor analysis**:

```{r}
model_COG =
"
  COG_PS =~ PROC_SPEED_vis_RT_med + PROC_SPEED_vis_dec_RT_med + PROC_SPEED_ac_RT_med
  COG_DS =~ digit_span_forward_span + digit_span_backward_span
  COG_nback =~ n_back_nback_1_dprime + n_back_nback_2_dprime + n_back_nback_3_dprime
  COG_RT =~ stroop_RT_score + simon_RT_score
"
```

**Model fits for CFA**:

```{r}
model_COG_fit = cfa(model_COG, df, missing = "ML", estimator = "MLR")
summary(model_COG_fit, standardized = TRUE, fit.measures = TRUE)
```

In three cases the unexplained variance is very high (\>0.65). 1. First, we excluded the *nback_1_dprime* variable, since it does not carry more information compared to *nback_2_dprime* and *nback_3_dprime* variables both theoretically and measurement-wise. 2. Secondly, we split the *COG_RT factor* into two, since the *stroop* and *simon* tasks have potentially additional exploratory power in the model. 3. Thirdly, we split the *COG_PS factor* into two. (For detail see next section.)

------------------------------------------------------------------------

To split the COG_PS into two factor, we performed two cfa analyses with nested models (*model_PS_01* and *model_PS_02*).

**First model / Model_PS_01**:

```{r}
model_PS_01 =
"
  PS_01 =~ PROC_SPEED_vis_RT_med + PROC_SPEED_vis_dec_RT_med
  PS_02 =~ PROC_SPEED_ac_RT_med
  COG_DS =~ digit_span_forward_span + digit_span_backward_span
  COG_nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
  COG_stroop =~ stroop_RT_score 
  COG_simon =~ simon_RT_score
"
```

```{r}
model_PS_01_fit = cfa(model_PS_01, df, missing = "ML", estimator = "MLR")
summary(model_PS_01_fit, standardized = TRUE, fit.measures = TRUE)
```

**Second model / Model_PS_02**:

```{r}
model_PS_02 =
"
  PS_01 =~ PROC_SPEED_vis_RT_med + PROC_SPEED_ac_RT_med
  PS_02 =~ PROC_SPEED_vis_dec_RT_med
  COG_DS =~ digit_span_forward_span + digit_span_backward_span
  COG_nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
  COG_stroop =~ stroop_RT_score 
  COG_simon =~ simon_RT_score
"
```

```{r}
model_PS_02_fit = cfa(model_PS_02, df, missing = "ML", estimator = "MLR")
summary(model_PS_02_fit, standardized = TRUE, fit.measures = TRUE)
```

Based on the two models, we concluded that *model_PS_02* indicates a better model considering four fit indices (TLI, CFI, SRMR, RMSEA) and the BIC. So for further analyses, we used the *PROC_SPEED_vis_dec_RT_med [processing speed - visual decision time]* index as a single-variable latent factor retaining the other two variables as the *PS factor*.

------------------------------------------------------------------------

The final grouping of the variables is the following structure:

1.  **Processing Speed**:

-   PROC_SPEED_vis_RT_med [processing speed - visual reaction time]
-   PROC_SPEED_ac_RT_med [processing speed - auditory reaction time]

2.  **Visual Decision Speed**:\

-   PROC_SPEED_vis_dec_RT_med [processing speed - visual decision time]

3.  **Digit Span**:

-   digit_span_forward_span [digit span - forward digit span]
-   digit_span_backward_span [digit span - backward digit span]

4.  **NBACK test**:

-   n_back_nback_2_dprime [n-back - 2-back d-prime]
-   n_back_nback_3_dprime [n-back - 3-back d-prime]

5.  **Stroop task**:

-   stroop_RT_score [stroop - RT]

6.  **Simon task**:

-   simon_RT_score [simon - RT]

## 2.4. Latent variables in language domain

**Kaiser-Meyer-Olkin test**:

```{r}
df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    predictive_ACC_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
KMO()
```

We excluded the *predictive_ACC_score [predictive sentence processing - accuracy]* index which showed unacceptable KMO values (\< 0.5).

```{r}
df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
KMO()
```

All indices are now in the acceptable range (\> 0.6), and the *predictive_RT_score [predictive sentence processing - RT]* is also near the boundary.

**Parallel analysis**:

```{r}
df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates 2 factors emerging.

**Exploratory factor analysis**:

```{r}
LANG_EFA = df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 2,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r}
summary(LANG_EFA)
```

```{r}
LANG_EFA$loadings
tidy(LANG_EFA$uniquenesses)
```

Since the 2-factor model showed a complex structure, we proceeded with hierarchical cluster analysis to reveal potential single-variable factors.

**Hierarchical cluster analysis**:

```{r}
tree_01 = df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
hclustvar()
```

```{r}
plot(tree_01)
```

```{r}
stab = stability(tree_01, B=100)
```

**K-means cluster analysis**:

```{r}
km = df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean

  ) %>% 
kmeansvar(init = 5, nstart = 1000)
```

```{r}
summary(km)
```

As a next step, we tested whether these factors show appropriate fit using cfa.

**Confirmatory factor analysis**:

```{r}
model_LANG =
"
  LANG_MENYET =~ MENYET_mean_ACC_all
  LANG_OMR =~ OMR_read_syls
  LANG_pred =~ predictive_RT_score
  LANG_space =~ selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
  LANG_TROG =~ trog_pragm_ACC_mean
"
```

**Fit indices for CFA**:

```{r}
model_LANG_fit = cfa(model_LANG, df, missing = "ML", estimator = "MLR")
summary(model_LANG_fit, standardized = TRUE, fit.measures = TRUE)
```

In conclusion, we used 5 clusters of language related variables to identify latent variables which serve as outcome in the SEM models.

The final structure is:

1.  **grammatical sensitivity**

-   MENYET_mean_ACC_all [grammatical sensitivity]

2.  **one minute reading**

-   OMR_read_syls [one-minute reading]

3.  **predictive sentence processing**

-   predictive_RT_score [predictive sentence processing - RT]

4.  **self-paced reading**

-   selfpaced_target_RT_diff_GP [self-paced reading - garden path sentence processing]
-   selfpaced_target_RT_diff_sertes [self-paced reading - sentence violation processing]

5.  **pragmatic comprehension**

-   trog_pragm_ACC_mean [KOBAK - pragmatic comprehension]

## 2.5. Summary of the latent variables

**Table 2**

*Latent variables and constituent indices*

| domain | index                             | latent variable                |
|------------------|----------------------------|--------------------------|
| SL     | SEGM RT TRN3--RND4                | SEGM RT                        |
| SL     | SEGM RT RND4--REC5                | SEGM RT                        |
| SL     | SEGM 2AFC bigram                  | SEGM 2AFC                      |
| SL     | SEGM 2AFC trigram                 | SEGM 2AFC                      |
| SL     | SEGM production                   | SEGM production                |
| SL     | AGL 2AFC sentence                 | AGL                            |
| SL     | AGL 2AFC production               | AGL                            |
| SL     | AGL phrase                        | AGL phrase                     |
| LANG   | grammatical sensitivity           | grammatical sensitivity        |
| LANG   | pragmatic comprehension           | pragmatic comprehension        |
| LANG   | garden path sentence processing   | self-paced reading             |
| LANG   | sentence violation processing     | self-paced reading             |
| LANG   | predictive sentence processing RT | predictive sentence processing |
| LANG   | one minute reading                | one minute reading             |
| COG    | visual reaction time              | processing speed               |
| COG    | auditory reaction time            | processing speed               |
| COG    | visual decision time              | visual decision speed          |
| COG    | forward digit span                | digit span                     |
| COG    | backward digit span               | digit span                     |
| COG    | 2-back d-prime                    | nback                          |
| COG    | 3-back d-prime                    | nback                          |
| COG    | Stroop RT                         | Stroop                         |
| COG    | Simon RT                          | Simon                          |

# 3. Mediation models (Structural equation modelling)

After the identification of latent factors, we proceeded with assembling the models for mediation analysis. In each model, there was one statistical learning variable, all the general cognitive variables, and the language variables. We specified the models with lavaan syntax. CFA estimator was Robust Maximum Likelihood method. Missing values were imputed using Maximum Likelihood method. An optimal model fit was ascertained based on Comparative Fit Index (CFI) and Tucker-Lewis Index (TLI) values ranging between 0.90--0.95, and an acceptable model fit was indicated by a Root Mean Squared Error of Approximation (RMSEA) index below 0.05 and Standardised Root Mean Square Residual (SRMR) index below 0.08. Inflated indices were expected due to single-item factors.

## 3.1. Calculating error variances from reliability indices

```{r}
# Error variances

stroop_e = (1-0.829)*var(df$stroop_RT_score, na.rm = TRUE)
simon_e = (1-0.724)*var(df$simon_RT_score, na.rm = TRUE)
visdec_e = (1-0.955)*var(df$PROC_SPEED_vis_dec_RT_med, na.rm = TRUE)
menyet_e = (1-0.661)*var(df$MENYET_mean_ACC_all, na.rm = TRUE)
pred_e = (1-0.866)*var(df$predictive_RT_score, na.rm = TRUE)
trog_e = (1-0.666)*var(df$trog_pragm_ACC_mean, na.rm = TRUE)
```

## 3.2. SEGM_RT model

```{r}
model_RT = 
"
# statistical learning
SL =~ 1*SEGM_AL_medRT_TRN3_RND4 + 1*SEGM_AL_medRT_RND4_REC5

# cognitive
PS =~ PROC_SPEED_vis_RT_med + PROC_SPEED_ac_RT_med
DS =~ digit_span_forward_span + digit_span_backward_span
nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
stroop =~ stroop_RT_score
simon =~ simon_RT_score
PS_visdec =~ PROC_SPEED_vis_dec_RT_med

# language
MENYET =~ MENYET_mean_ACC_all
pred =~ predictive_RT_score
space =~ selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
OMR =~ OMR_read_syls
TROG =~ trog_pragm_ACC_mean

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT_score ~~ 0.152*stroop_RT_score
simon_RT_score ~~ 0.274*simon_RT_score
PROC_SPEED_vis_dec_RT_med ~~ 0.043*PROC_SPEED_vis_dec_RT_med
MENYET_mean_ACC_all ~~ 0.334*MENYET_mean_ACC_all
predictive_RT_score ~~ 0.136*predictive_RT_score
trog_pragm_ACC_mean ~~ 0.326*trog_pragm_ACC_mean
"
```

```{r}
model_fit_RT = sem(model_RT, df, missing = "ML", estimator = "MLR")
summary(model_fit_RT, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.3. SEGM_2_AFC model

```{r}
model_SEGM_tribi = 
"
# statistical learning
SL =~ SEGM_AL_2AFC_trigram + SEGM_AL_2AFC_bigram

# cognitive
PS =~ PROC_SPEED_vis_RT_med + PROC_SPEED_ac_RT_med
DS =~ digit_span_forward_span + digit_span_backward_span
nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
stroop =~ stroop_RT_score
simon =~ simon_RT_score
PS_visdec =~ PROC_SPEED_vis_dec_RT_med

# language
MENYET =~ MENYET_mean_ACC_all
pred =~ predictive_RT_score
space =~ selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
OMR =~ OMR_read_syls
TROG =~ trog_pragm_ACC_mean

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT_score ~~ 0.152*stroop_RT_score
simon_RT_score ~~ 0.274*simon_RT_score
PROC_SPEED_vis_dec_RT_med ~~ 0.043*PROC_SPEED_vis_dec_RT_med
MENYET_mean_ACC_all ~~ 0.334*MENYET_mean_ACC_all
predictive_RT_score ~~ 0.136*predictive_RT_score
trog_pragm_ACC_mean ~~ 0.326*trog_pragm_ACC_mean
"
```

```{r}
model_fit_SEGM_tribi = sem(model_SEGM_tribi, df, missing = "ML", estimator = "MLR")
summary(model_fit_SEGM_tribi, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.4. SEGM_production model

```{r}
model_SEGM_prod = 
"
# statistical learning
SL =~ SEGM_AL_SEGM_prod_data

# cognitive
PS =~ PROC_SPEED_vis_RT_med + PROC_SPEED_ac_RT_med
DS =~ digit_span_forward_span + digit_span_backward_span
nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
stroop =~ stroop_RT_score
simon =~ simon_RT_score
PS_visdec =~ PROC_SPEED_vis_dec_RT_med

# language
MENYET =~ MENYET_mean_ACC_all
pred =~ predictive_RT_score
space =~ selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
OMR =~ OMR_read_syls
TROG =~ trog_pragm_ACC_mean

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT_score ~~ 0.152*stroop_RT_score
simon_RT_score ~~ 0.274*simon_RT_score
PROC_SPEED_vis_dec_RT_med ~~ 0.043*PROC_SPEED_vis_dec_RT_med
MENYET_mean_ACC_all ~~ 0.334*MENYET_mean_ACC_all
predictive_RT_score ~~ 0.136*predictive_RT_score
trog_pragm_ACC_mean ~~ 0.326*trog_pragm_ACC_mean
"
```

```{r}
model_fit_SEGM_prod = sem(model_SEGM_prod, df, missing = "ML", estimator = "MLR")
summary(model_fit_SEGM_prod, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.5. AGL_model

```{r}
model_AGL = 
"
# statistical learning
SL =~ AGL_2AFC_sent + AGL_prod

# cognitive
PS =~ PROC_SPEED_vis_RT_med + PROC_SPEED_ac_RT_med
DS =~ digit_span_forward_span + digit_span_backward_span
nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
stroop =~ stroop_RT_score
simon =~ simon_RT_score
PS_visdec =~ PROC_SPEED_vis_dec_RT_med

# language
MENYET =~ MENYET_mean_ACC_all
pred =~ predictive_RT_score
space =~ selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
OMR =~ OMR_read_syls
TROG =~ trog_pragm_ACC_mean

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT_score ~~ 0.152*stroop_RT_score
simon_RT_score ~~ 0.274*simon_RT_score
PROC_SPEED_vis_dec_RT_med ~~ 0.043*PROC_SPEED_vis_dec_RT_med
MENYET_mean_ACC_all ~~ 0.334*MENYET_mean_ACC_all
predictive_RT_score ~~ 0.136*predictive_RT_score
trog_pragm_ACC_mean ~~ 0.326*trog_pragm_ACC_mean
"
```

```{r}
model_fit_AGL = sem(model_AGL, df, missing = "ML", estimator = "MLR")
summary(model_fit_AGL, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.6. AGL_phrase model

```{r}
model_phr = 
"
# statistical learning
SL =~ AGL_2AFC_phr

# cognitive
PS =~ PROC_SPEED_vis_RT_med + PROC_SPEED_ac_RT_med
DS =~ digit_span_forward_span + digit_span_backward_span
nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
stroop =~ stroop_RT_score
simon =~ simon_RT_score
PS_visdec =~ PROC_SPEED_vis_dec_RT_med

# language
MENYET =~ MENYET_mean_ACC_all
pred =~ predictive_RT_score
space =~ selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
OMR =~ OMR_read_syls
TROG =~ trog_pragm_ACC_mean

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL
TROG ~ c51*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
simon ~ a21*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ MENYET
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# single indicator factors

stroop_RT_score ~~ 0.152*stroop_RT_score
simon_RT_score ~~ 0.274*simon_RT_score
PROC_SPEED_vis_dec_RT_med ~~ 0.043*PROC_SPEED_vis_dec_RT_med
MENYET_mean_ACC_all ~~ 0.334*MENYET_mean_ACC_all
predictive_RT_score ~~ 0.136*predictive_RT_score
trog_pragm_ACC_mean ~~ 0.326*trog_pragm_ACC_mean
"
```

```{r}
model_fit_phr = sem(model_phr, df, missing = "ML", estimator = "MLR")
summary(model_fit_phr, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## 3.7. Unified model

We also tested a unified model With all the variables for calculating total effect sizes.

```{r}
model_full =
"
# statistical learning
SL_RT =~ SEGM_AL_medRT_TRN3_RND4 + SEGM_AL_medRT_RND4_REC5
SL_AGL =~ AGL_2AFC_sent + AGL_prod
SL_SEGM_prod =~ SEGM_AL_SEGM_prod_data
SL_SEGM_tribi =~ SEGM_AL_2AFC_trigram + SEGM_AL_2AFC_bigram
SL_phr =~ AGL_2AFC_phr

# cognitive
PS =~ PROC_SPEED_vis_RT_med + PROC_SPEED_ac_RT_med
DS =~ digit_span_forward_span + digit_span_backward_span
nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
stroop =~ stroop_RT_score
simon =~ simon_RT_score
PS_visdec =~ PROC_SPEED_vis_dec_RT_med

# language
MENYET =~ MENYET_mean_ACC_all
pred =~ predictive_RT_score
space =~ selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
OMR =~ OMR_read_syls
TROG =~ trog_pragm_ACC_mean

# dependent regression
MENYET ~ b11*DS + b12*simon + b13*PS_visdec + b14*nback + b15*PS + b16*stroop + c11*SL_RT + c12*SL_AGL + c13*SL_SEGM_prod + c14*SL_SEGM_tribi + c15*SL_phr
pred ~ b21*DS + b22*simon + b23*PS_visdec + b24*nback + b25*PS + b26*stroop + c21*SL_RT + c22*SL_AGL + c23*SL_SEGM_prod + c24*SL_SEGM_tribi + c25*SL_phr
space ~ b31*DS + b32*simon + b33*PS_visdec + b34*nback + b35*PS + b36*stroop + c31*SL_RT + c32*SL_AGL + c33*SL_SEGM_prod + c34*SL_SEGM_tribi + c35*SL_phr
OMR ~ b41*DS + b42*simon + b43*PS_visdec + b44*nback + b45*PS + b46*stroop + c41*SL_RT + c42*SL_AGL + c43*SL_SEGM_prod + c44*SL_SEGM_tribi + c45*SL_phr
TROG ~ b51*DS + b52*simon + b53*PS_visdec + b54*nback + b55*PS + b56*stroop + c51*SL_RT + c52*SL_AGL + c53*SL_SEGM_prod + c54*SL_SEGM_tribi + c55*SL_phr

# mediator regression
DS ~ a11*SL_RT + a12*SL_AGL + a13*SL_SEGM_prod + a14*SL_SEGM_tribi + a15*SL_phr
simon ~ a21*SL_RT + a22*SL_AGL + a23*SL_SEGM_prod + a24*SL_SEGM_tribi + a25*SL_phr
PS_visdec ~ a31*SL_RT + a32*SL_AGL + a33*SL_SEGM_prod + a34*SL_SEGM_tribi + a35*SL_phr
nback ~ a41*SL_RT + a42*SL_AGL + a43*SL_SEGM_prod + a44*SL_SEGM_tribi + a45*SL_phr
PS ~ a51*SL_RT + a52*SL_AGL + a53*SL_SEGM_prod + a54*SL_SEGM_tribi + a55*SL_phr
stroop ~ a61*SL_RT + a62*SL_AGL + a63*SL_SEGM_prod + a64*SL_SEGM_tribi + a65*SL_phr

# mediator residual covariance
DS ~~ simon
DS ~~ PS_visdec
simon ~~ PS_visdec
DS ~~ nback
simon ~~ nback
PS_visdec ~~ nback
DS ~~ PS
simon ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
simon ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR
TROG ~~ space
TROG ~~ pred
TROG ~~ OMR
TROG ~~ MENYET

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m2_y1 := a21*b12
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m2_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y1 ~ x2
ind_x2_m1_y1 := a12*b11
ind_x2_m2_y1 := a22*b12
ind_x2_m3_y1 := a32*b13
ind_x2_m4_y1 := a42*b14
ind_x2_m5_y1 := a52*b15
ind_x2_m6_y1 := a62*b16
ind_x2_y1 := ind_x2_m1_y1 + ind_x2_m2_y1 + ind_x2_m3_y1 + ind_x2_m4_y1 + ind_x2_m5_y1 + ind_x2_m6_y1
tot_x2_y1 := ind_x2_y1 + c12

# y1 ~ x3
ind_x3_m1_y1 := a13*b11
ind_x3_m2_y1 := a23*b12
ind_x3_m3_y1 := a33*b13
ind_x3_m4_y1 := a43*b14
ind_x3_m5_y1 := a53*b15
ind_x3_m6_y1 := a63*b16
ind_x3_y1 := ind_x3_m1_y1 + ind_x3_m2_y1 + ind_x3_m3_y1 + ind_x3_m4_y1 + ind_x3_m5_y1 + ind_x3_m6_y1
tot_x3_y1 := ind_x3_y1 + c13

# y1 ~ x4
ind_x4_m1_y1 := a14*b11
ind_x4_m2_y1 := a24*b12
ind_x4_m3_y1 := a34*b13
ind_x4_m4_y1 := a44*b14
ind_x4_m5_y1 := a54*b15
ind_x4_m6_y1 := a64*b16
ind_x4_y1 := ind_x4_m1_y1 + ind_x4_m2_y1 + ind_x4_m3_y1 + ind_x4_m4_y1 + ind_x4_m5_y1 + ind_x4_m6_y1
tot_x4_y1 := ind_x4_y1 + c14

# y1 ~ x5
ind_x5_m1_y1 := a15*b11
ind_x5_m2_y1 := a25*b12
ind_x5_m3_y1 := a35*b13
ind_x5_m4_y1 := a45*b14
ind_x5_m5_y1 := a55*b15
ind_x5_m6_y1 := a65*b16
ind_x5_y1 := ind_x5_m1_y1 + ind_x5_m2_y1 + ind_x5_m3_y1 + ind_x5_m4_y1 + ind_x5_m5_y1 + ind_x5_m6_y1
tot_x5_y1 := ind_x5_y1 + c15

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m2_y2 := a21*b22
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m2_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y2 ~ x2
ind_x2_m1_y2 := a12*b21
ind_x2_m2_y2 := a22*b22
ind_x2_m3_y2 := a32*b23
ind_x2_m4_y2 := a42*b24
ind_x2_m5_y2 := a52*b25
ind_x2_m6_y2 := a62*b26
ind_x2_y2 := ind_x2_m1_y2 + ind_x2_m2_y2 + ind_x2_m3_y2 + ind_x2_m4_y2 + ind_x2_m5_y2 + ind_x2_m6_y2
tot_x2_y2 := ind_x2_y2 + c22

# y2 ~ x3
ind_x3_m1_y2 := a13*b21
ind_x3_m2_y2 := a23*b22
ind_x3_m3_y2 := a33*b23
ind_x3_m4_y2 := a43*b24
ind_x3_m5_y2 := a53*b25
ind_x3_m6_y2 := a63*b26
ind_x3_y2 := ind_x3_m1_y2 + ind_x3_m2_y2 + ind_x3_m3_y2 + ind_x3_m4_y2 + ind_x3_m5_y2 + ind_x3_m6_y2
tot_x3_y2 := ind_x3_y2 + c23

# y2 ~ x4
ind_x4_m1_y2 := a14*b21
ind_x4_m2_y2 := a24*b22
ind_x4_m3_y2 := a34*b23
ind_x4_m4_y2 := a44*b24
ind_x4_m5_y2 := a54*b25
ind_x4_m6_y2 := a64*b26
ind_x4_y2 := ind_x4_m1_y2 + ind_x4_m2_y2 + ind_x4_m3_y2 + ind_x4_m4_y2 + ind_x4_m5_y2 + ind_x4_m6_y2
tot_x4_y2 := ind_x4_y2 + c24

# y2 ~ x5
ind_x5_m1_y2 := a15*b21
ind_x5_m2_y2 := a25*b22
ind_x5_m3_y2 := a35*b23
ind_x5_m4_y2 := a45*b24
ind_x5_m5_y2 := a55*b25
ind_x5_m6_y2 := a65*b26
ind_x5_y2 := ind_x5_m1_y2 + ind_x5_m2_y2 + ind_x5_m3_y2 + ind_x5_m4_y2 + ind_x5_m5_y2 + ind_x5_m6_y2
tot_x5_y2 := ind_x5_y2 + c25

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m2_y3 := a21*b32
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m2_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y3 ~ x2
ind_x2_m1_y3 := a12*b31
ind_x2_m2_y3 := a22*b32
ind_x2_m3_y3 := a32*b33
ind_x2_m4_y3 := a42*b34
ind_x2_m5_y3 := a52*b35
ind_x2_m6_y3 := a62*b36
ind_x2_y3 := ind_x2_m1_y3 + ind_x2_m2_y3 + ind_x2_m3_y3 + ind_x2_m4_y3 + ind_x2_m5_y3 + ind_x2_m6_y3
tot_x2_y3 := ind_x2_y3 + c32

# y3 ~ x3
ind_x3_m1_y3 := a13*b31
ind_x3_m2_y3 := a23*b32
ind_x3_m3_y3 := a33*b33
ind_x3_m4_y3 := a43*b34
ind_x3_m5_y3 := a53*b35
ind_x3_m6_y3 := a63*b36
ind_x3_y3 := ind_x3_m1_y3 + ind_x3_m2_y3 + ind_x3_m3_y3 + ind_x3_m4_y3 + ind_x3_m5_y3 + ind_x3_m6_y3
tot_x3_y3 := ind_x3_y3 + c33

# y3 ~ x4
ind_x4_m1_y3 := a14*b31
ind_x4_m2_y3 := a24*b32
ind_x4_m3_y3 := a34*b33
ind_x4_m4_y3 := a44*b34
ind_x4_m5_y3 := a54*b35
ind_x4_m6_y3 := a64*b36
ind_x4_y3 := ind_x4_m1_y3 + ind_x4_m2_y3 + ind_x4_m3_y3 + ind_x4_m4_y3 + ind_x4_m5_y3 + ind_x4_m6_y3
tot_x4_y3 := ind_x4_y3 + c34

# y3 ~ x5
ind_x5_m1_y3 := a15*b31
ind_x5_m2_y3 := a25*b32
ind_x5_m3_y3 := a35*b33
ind_x5_m4_y3 := a45*b34
ind_x5_m5_y3 := a55*b35
ind_x5_m6_y3 := a65*b36
ind_x5_y3 := ind_x5_m1_y3 + ind_x5_m2_y3 + ind_x5_m3_y3 + ind_x5_m4_y3 + ind_x5_m5_y3 + ind_x5_m6_y3
tot_x5_y3 := ind_x5_y3 + c35

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m2_y4 := a21*b42
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m2_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y4 ~ x2
ind_x2_m1_y4 := a12*b41
ind_x2_m2_y4 := a22*b42
ind_x2_m3_y4 := a32*b43
ind_x2_m4_y4 := a42*b44
ind_x2_m5_y4 := a52*b45
ind_x2_m6_y4 := a62*b46
ind_x2_y4 := ind_x2_m1_y4 + ind_x2_m2_y4 + ind_x2_m3_y4 + ind_x2_m4_y4 + ind_x2_m5_y4 + ind_x2_m6_y4
tot_x2_y4 := ind_x2_y4 + c42

# y4 ~ x3
ind_x3_m1_y4 := a13*b41
ind_x3_m2_y4 := a23*b42
ind_x3_m3_y4 := a33*b43
ind_x3_m4_y4 := a43*b44
ind_x3_m5_y4 := a53*b45
ind_x3_m6_y4 := a63*b46
ind_x3_y4 := ind_x3_m1_y4 + ind_x3_m2_y4 + ind_x3_m3_y4 + ind_x3_m4_y4 + ind_x3_m5_y4 + ind_x3_m6_y4
tot_x3_y4 := ind_x3_y4 + c43

# y4 ~ x4
ind_x4_m1_y4 := a14*b41
ind_x4_m2_y4 := a24*b42
ind_x4_m3_y4 := a34*b43
ind_x4_m4_y4 := a44*b44
ind_x4_m5_y4 := a54*b45
ind_x4_m6_y4 := a64*b46
ind_x4_y4 := ind_x4_m1_y4 + ind_x4_m2_y4 + ind_x4_m3_y4 + ind_x4_m4_y4 + ind_x4_m5_y4 + ind_x4_m6_y4
tot_x4_y4 := ind_x4_y4 + c44

# y4 ~ x5
ind_x5_m1_y4 := a15*b41
ind_x5_m2_y4 := a25*b42
ind_x5_m3_y4 := a35*b43
ind_x5_m4_y4 := a45*b44
ind_x5_m5_y4 := a55*b45
ind_x5_m6_y4 := a65*b46
ind_x5_y4 := ind_x5_m1_y4 + ind_x5_m2_y4 + ind_x5_m3_y4 + ind_x5_m4_y4 + ind_x5_m5_y4 + ind_x5_m6_y4
tot_x5_y4 := ind_x5_y4 + c45

# y5 ~ x1
ind_x1_m1_y5 := a11*b51
ind_x1_m2_y5 := a21*b52
ind_x1_m3_y5 := a31*b53
ind_x1_m4_y5 := a41*b54
ind_x1_m5_y5 := a51*b55
ind_x1_m6_y5 := a61*b56
ind_x1_y5 := ind_x1_m1_y5 + ind_x1_m2_y5 + ind_x1_m3_y5 + ind_x1_m4_y5 + ind_x1_m5_y5 + ind_x1_m6_y5
tot_x1_y5 := ind_x1_y5 + c51

# y5 ~ x2
ind_x2_m1_y5 := a12*b51
ind_x2_m2_y5 := a22*b52
ind_x2_m3_y5 := a32*b53
ind_x2_m4_y5 := a42*b54
ind_x2_m5_y5 := a52*b55
ind_x2_m6_y5 := a62*b56
ind_x2_y5 := ind_x2_m1_y5 + ind_x2_m2_y5 + ind_x2_m3_y5 + ind_x2_m4_y5 + ind_x2_m5_y5 + ind_x2_m6_y5
tot_x2_y5 := ind_x2_y5 + c52

# y5 ~ x3
ind_x3_m1_y5 := a13*b51
ind_x3_m2_y5 := a23*b52
ind_x3_m3_y5 := a33*b53
ind_x3_m4_y5 := a43*b54
ind_x3_m5_y5 := a53*b55
ind_x3_m6_y5 := a63*b56
ind_x3_y5 := ind_x3_m1_y5 + ind_x3_m2_y5 + ind_x3_m3_y5 + ind_x3_m4_y5 + ind_x3_m5_y5 + ind_x3_m6_y5
tot_x3_y5 := ind_x3_y5 + c53

# y5 ~ x4
ind_x4_m1_y5 := a14*b51
ind_x4_m2_y5 := a24*b52
ind_x4_m3_y5 := a34*b53
ind_x4_m4_y5 := a44*b54
ind_x4_m5_y5 := a54*b55
ind_x4_m6_y5 := a64*b56
ind_x4_y5 := ind_x4_m1_y5 + ind_x4_m2_y5 + ind_x4_m3_y5 + ind_x4_m4_y5 + ind_x4_m5_y5 + ind_x4_m6_y5
tot_x4_y5 := ind_x4_y5 + c54

# y5 ~ x5
ind_x5_m1_y5 := a15*b51
ind_x5_m2_y5 := a25*b52
ind_x5_m3_y5 := a35*b53
ind_x5_m4_y5 := a45*b54
ind_x5_m5_y5 := a55*b55
ind_x5_m6_y5 := a65*b56
ind_x5_y5 := ind_x5_m1_y5 + ind_x5_m2_y5 + ind_x5_m3_y5 + ind_x5_m4_y5 + ind_x5_m5_y5 + ind_x5_m6_y5
tot_x5_y5 := ind_x5_y5 + c55

# y ~ x(all)

ind_y1 := ind_x1_y1 + ind_x2_y1 + ind_x3_y1 + ind_x4_y1 + ind_x5_y1
tot_y1 := tot_x1_y1 + tot_x2_y1 + tot_x3_y1 + tot_x4_y1 + tot_x5_y1

ind_y2 := ind_x1_y2 + ind_x2_y2 + ind_x3_y2 + ind_x4_y2 + ind_x5_y2
tot_y2 := tot_x1_y2 + tot_x2_y2 + tot_x3_y2 + tot_x4_y2 + tot_x5_y2

ind_y3 := ind_x1_y3 + ind_x2_y3 + ind_x3_y3 + ind_x4_y3 + ind_x5_y3
tot_y3 := tot_x1_y3 + tot_x2_y3 + tot_x3_y3 + tot_x4_y3 + tot_x5_y3

ind_y4 := ind_x1_y4 + ind_x2_y4 + ind_x3_y4 + ind_x4_y4 + ind_x5_y4
tot_y4 := tot_x1_y4 + tot_x2_y4 + tot_x3_y4 + tot_x4_y4 + tot_x5_y4

ind_y5 := ind_x1_y5 + ind_x2_y5 + ind_x3_y5 + ind_x4_y5 + ind_x5_y5
tot_y5 := tot_x1_y5 + tot_x2_y5 + tot_x3_y5 + tot_x4_y5 + tot_x5_y5

dir_y1 := tot_y1 - ind_y1
dir_y2 := tot_y2 - ind_y2
dir_y3 := tot_y3 - ind_y3
dir_y4 := tot_y4 - ind_y4
dir_y5 := tot_y5 - ind_y5

# single indicator factors

stroop_RT_score ~~ 0.152*stroop_RT_score
simon_RT_score ~~ 0.274*simon_RT_score
PROC_SPEED_vis_dec_RT_med ~~ 0.043*PROC_SPEED_vis_dec_RT_med
MENYET_mean_ACC_all ~~ 0.334*MENYET_mean_ACC_all
predictive_RT_score ~~ 0.136*predictive_RT_score
"
```

```{r}
model_fit_full = sem(model_full, df, missing = "ML", estimator = "MLR")
summary(model_fit_full, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

## Summary of mediation models

**Table 3**

*Fit indices of mediation models*

| model           | CFI  | R-CFI | TLI  | R-TLI | R-RMSEA | SRMR |
|-----------------|------|-------|------|-------|-------|------|
| SEGM RT         | .983 | .976  | .961 | .946  | .040  | .028 |
| SEGM 2AFC       | .992 | .999  | .983 | .999  | .006  | .024 |
| SEGM production | .991 | .996  | .975 | .989  | .018  | .022 |
| AGL             | .989 | .995  | .975 | .988  | .018  | .025 |
| AGL phrase      | .988 | .992  | .969 | .979  | .024  | .024 |
| SL overall      | .988 | .985  | .974 | .970  | .025  | .032 |

*Note.* CFI: = Comparative Fit Index, R-CFI: = Robust Comparative Fit Index, TLI: = Tucker-Lewis Index, R-TLI: = Robust Tucker-Lewis Index, R-RMSEA: = Robust Root Mean Square Error of Approximation, SRMR: = Standardised Root Mean Square Residual.


**Table 4**

*Reliabilities and average variance extracted (AVE) for latent variables*

```{r}
reliability(model_fit_RT)
reliability(model_fit_SEGM_tribi)
reliability(model_fit_SEGM_prod)
reliability(model_fit_AGL)
reliability(model_fit_phr)
reliability(model_fit_full)
```


**Table 5**

*Mediation models*

| predictor                      | outcome                 | direct effect | indirect effect | total effect |
|---------------|---------------|---------------|---------------|---------------|
| SEGM RT                        | grammatical sensitivity | -.115         | .114\*          | -.001        |
| predictive sentence processing | -.071                   | .068\*        | -.003           |              |
| self-paced reading             | .046                    | .128\*\*      | .174            |              |
| one-minute reading             | .065                    | .094\*        | .159\*          |              |
| pragmatic comprehension        | .130                    | .101\*        | .231\*\*        |              |
| SEGM 2AFC                      | grammatical sensitivity | .394\*        | .099\*          | .493\*\*     |
| predictive sentence processing | .099                    | .074          | .174            |              |
| self-paced reading             | .033                    | .166\*\*      | .198            |              |
| one-minute reading             | -.151                   | .157\*\*      | .005            |              |
| pragmatic comprehension        | .144                    | .118\*        | .262\*          |              |
| SEGM production                | grammatical sensitivity | .304\*\*      | .145\*\*        | .450\*\*\*   |
| predictive sentence processing | -.047                   | .136\*\*      | .088            |              |
| self-paced reading             | -.062                   | .193\*\*      | .132            |              |
| one-minute reading             | .114                    | .111\*        | .225\*\*\*      |              |
| pragmatic comprehension        | .258\*\*                | .138\*\*      | .396\*\*\*      |              |
| AGL                            | grammatical sensitivity | .143          | .233\*          | .376\*\*\*   |
| predictive sentence processing | .156                    | .114          | .269\*\*        |              |
| self-paced reading             | .077                    | .213\*        | .290\*          |              |
| one-minute reading             | .259                    | .072          | .331\*\*\*      |              |
| pragmatic comprehension        | .446\*                  | .069          | .515\*\*\*      |              |
| AGL phrase                     | grammatical sensitivity | -.018         | .125\*\*        | .106         |
| predictive sentence processing | .031                    | .062\*        | .093            |              |
| self-paced reading             | .128                    | .071          | .199            |              |
| one-minute reading             | -.072                   | .073\*        | .002            |              |
| pragmatic comprehension        | .109                    | .081\*        | .190\*          |              |
| SL overall                     | grammatical sensitivity | .336          | .215            | .551\*\*\*   |
| predictive sentence processing | .079                    | .174          | .253\*\*        |              |
| self-paced reading             | .152                    | .340\*        | .491\*\*        |              |
| one-minute reading             | .191                    | .117          | .308\*\*        |              |
| pragmatic comprehension        | .625                    | -.009         | .617\*\*\*      |              |

*Note.* \*: p \< .05, \*\*: p \< .01, \*\*\*: p \< .001
