---
editor_options: 
  markdown: 
    wrap: 72
---

|                                                     |
|-----------------------------------------------------|
| title: "A purely data-driven model and its caveats" |
| output: html_notebook                               |

# Preparation

## R packages:

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(psych)
library(EFAtools)
library(lavaan)
library(ClustOfVar)
library(semTools)
library(semPlot)
library(tidySEM)
```

## Data:

```{r include = FALSE, warning = FALSE}
library(readr)
df = read_delim("df.csv", 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)
```

```{r message = FALSE, warning = FALSE}
df = df %>% 
  mutate(
    perceptual_speed_visual_RT = -(perceptual_speed_visual_RT),
    perceptual_speed_visual_decision = -(perceptual_speed_visual_decision),
    perceptual_speed_auditory_RT = -(perceptual_speed_auditory_RT),
    simon_RT = -(simon_RT),
    simon_accuracy = -(simon_accuracy),
    stroop_RT = -(stroop_RT),
    stroop_accuracy = -(stroop_accuracy),
    selfpaced_gardenpath_target = -(selfpaced_gardenpath_target),
    selfpaced_violation_processing_target = -(selfpaced_violation_processing_target)
  )
```

# 1. Latent factor identification

## 1.1. Summary for the process of latent factor identification

(See details below)

To identify latent factors wtih a purely data-driven approach, we did
not categorised the measurement indices into domains, as we have done so
in our main model. Instead, we allowed all indices in the same
statistical models.

Our classification method was the following:

1.  **KMO test**: Using the Kaiser-Meyer-Olkin test, we assessed the
    measures of sampling adequacy (MSA) for the indices. Subsequently,
    indices with unsatisfactory fit for EFA (MSA \< 0.6) were excluded.

2.  **Bartlett's test**: Secondly, Bartlett's test of sphericity was
    employed, with the significance level indicating a suitable fit for
    EFA.

3.  **Parallel analysis**: To determine the number of factors in each
    domain, Horn's parallel analysis was used. This method compares
    eigenvalues derived from the data matrix to those from a Monte Carlo
    simulated matrix generated from random data of the same size.

4.  **Exploratory factor analysis**: After determining the number of
    factors, exploratory factor analysis was conducted using maximum
    likelihood as the factoring method and promax (oblique) rotation.
    Missing values were imputed using the mean. Based on factor loadings
    and uniqueness observed in each index, we refined our model.

5.  **Hierarchical cluster analysis**: To validate EFA results,
    hierarchical cluster analyses were performed. The stability of
    partitions from a hierarchy of variables was evaluated using a
    bootstrapping method (n=100). Based on the mean adjusted Rand
    criterion, we examined local maximums regarding the potential number
    of clusters and selected the one supported by our theoretical
    considerations if necessary. We then proceeded with k-means
    clustering of variables, resulting in the identification of latent
    variables.

## 1.2. Latent factors without domain specification

**Kaiser-Meyer-Olkin test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_ACC_training,
    SEGM_ACC_TRN3_RND4,
    SEGM_ACC_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_ACC_training,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production,
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
    simon_RT,
    simon_accuracy,
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    predictive_sent_proc_accuracy,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
KMO()
```

The previously identified indices with unsatisfactory fit for EFA (MSA
\< 0.5) show similarly worse fit in this model resulting in their
exclusion: - SEGM_ACC_training [speech segmentation - SEGM ACC
training] - SEGM_ACC_TRN3_RND4 [speech segmentation - SEGM ACC
TRN3-RND4] - SEGM_ACC_RND4_REC5 [speech segmentation - SEGM ACC
RND4-REC5] - AGL_ACC_training [artificial grammar learning - AGL ACC
training]

After their exclusion, KMO test was repeated.

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production,
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
    simon_RT,
    simon_accuracy,
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    predictive_sent_proc_accuracy,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
KMO()
```

The reaction time based indices showed poor fit (\< 0.6), however, we
did not omit them for their possible exploratory power. To stick to the
data-driven approach, we did not divided the variables into neither
domains, nor subclasses (i.e. RT vs. powertests).

**Bartlett's test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production,
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
    simon_RT,
    simon_accuracy,
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    predictive_sent_proc_accuracy,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production,
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
    simon_RT,
    simon_accuracy,
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    predictive_sent_proc_accuracy,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis suggests that the number of factors formed should be
10. Notice that based on the scree plot, one factor emerges as a crucial
latent variable. This could be due to the well-described phenomenon of
the positive manifold in cognitive abilities in which the solitary
factor would be the g factor (independent of whether we see it as a
formative or reflective variable, for further details see Process
Overlap Theory (Kovacs & Conway, 2016))

**Exploratory factor analysis**:

```{r echo = FALSE, warning = FALSE}
allvar_01 = df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production,
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
    simon_RT,
    simon_accuracy,
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    predictive_sent_proc_accuracy,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 10,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r echo = FALSE, warning = FALSE}
summary(allvar_01)
```

```{r echo = FALSE, warning = FALSE}
allvar_01$loadings
tidy(allvar_01$uniquenesses)
```

The exploratory factor analysis with 10 factors shows a similar pattern
of small clusters of indices. Based on a loose criterion of factor
loadings \> 4, the analysis suggests the following latent variables. one
minute reading

| theoretical domain | index                           | data driven latent factor |
|-------------------|----------------------------|--------------------------|
| SL                 | SEGM RT TRN3--RND4              | SEGM RT                   |
| SL                 | SEGM RT RND4--REC5              | SEGM RT                   |
| SL                 | SEGM RT train                   | SEGM RT                   |
| SL                 | SEGM 2AFC bigram                | SEGM 2AFC                 |
| SL                 | SEGM 2AFC trigram               | SEGM 2AFC                 |
| SL                 | SEGM production                 | SEGM production           |
| SL                 | AGL 2AFC sentence               | AGL                       |
| SL                 | AGL 2AFC production             | AGL                       |
| SL                 | AGL phrase                      | AGL                       |
| LANG               | garden path sentence processing | reading                   |
| LANG               | sentence violation processing   | reading                   |
| LANG               | one minute reading              | reading                   |
| COG                | visual reaction time            | perceptual speed          |
| COG                | auditory reaction time          | perceptual speed          |
| COG                | visual decision time            | perceptual speed          |
| COG                | forward digit span              | digit span                |
| COG                | backward digit span             | digit span                |
| COG                | 2-back d-prime                  | nback                     |
| COG                | 3-back d-prime                  | nback                     |
| COG                | Stroop RT                       | Stroop                    |

Notable difference of this structure and the main model is that some of
the language indices (i.e. grammatical sensitivity, predictive sentence
processing accuracy, predictive sentence processing RT, pragmatic
comprehension) did not get sorted into any factors, as well as some
other (albeit poorly functioning) indices. If we assign these missing
indices to the own latent variables, we can conclude a similar
underlying factor structure. Furthermore, after assessing the potential
content of the factors, we can interpret the factor correlations from
the above EFA model. We can see a cluster of correlating non-linguistic
cognitive factors (perceptual speed, nback, digit span), as well as
statistical learning factors (AGL, SEGM production, SEGM 2AFC). However,
we can draw no definitive conclusions regarding the wider domains based
solely on data, as we can see high correlations between inter-domain
factors: e.g., perceptual speed - AGL 0.49; perceptual speed - reading
0.48; SEGM production - stroop 0.41.

To assess the hierarchical structure of all indices, we performed
hierarchical cluster analysis as well.

**Hierarchical cluster analysis**:

```{r echo = FALSE, warning = FALSE}
tree_01 = data.frame(df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production,
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
    simon_RT,
    simon_accuracy,
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    predictive_sent_proc_accuracy,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  )) %>% 
hclustvar()
```

```{r echo = FALSE, warning = FALSE}
plot(tree_01)
```

```{r echo = FALSE, warning = FALSE}
stab = stability(tree_01, B = 100)
```

Since the Rand criterion shows that there is two local maxima (at 11-12
and 17) regarding the number of clusters, we selected the smaller number
to get more information about the relationship of the indices.

**K-means cluster analysis**:

```{r echo = FALSE, warning = FALSE}
km = data.frame(df %>% 
  select(
    SEGM_RT_training,
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production,
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
    simon_RT,
    simon_accuracy,
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    predictive_sent_proc_accuracy,
    selfpaced_gardenpath_target,
    selfpaced_violation_processing_target,
    pragmatic_comprehension
  )) %>% 
kmeansvar(init = 12, nstart = 1000)
```

```{r echo = FALSE, warning = FALSE}
summary(km)
```

In conclusion, the hierarchical cluster shows similar clusters to both
the novel EFA results and to the main model on a fine-grained level.
However, the higher we go in the tree, the more mixed up the structure
is. Notable, the indices of the three domains get scrambled, for
instance SEGM production is closest to grammatical sensitivity, the AGL
indices are closest to digit span indices. The results suggest an
inconclusive higher level structure, which would prevent us from
conducting either an ordinary latent variable regression or a more
complex mediation analysis.
