---
title: "R Notebook"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

# 1. Setup and data cleaning

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(psych)
library(EFAtools)
library(lavaan)
library(ClustOfVar)
library(semTools)
library(semPlot)
library(tidySEM)
```

```{r include = FALSE, warning = FALSE}
library(readr)
df = read_csv("GitHub/lendulet_language_SL/docs/data/df.csv")
```

```{r message = FALSE, warning = FALSE}
df = df %>% 
  mutate(
    perceptual_speed_visual_RT = -(perceptual_speed_visual_RT),
    perceptual_speed_visual_decision = -(perceptual_speed_visual_decision),
    perceptual_speed_auditory_RT = -(perceptual_speed_auditory_RT),
    simon_RT = -(simon_RT),
    simon_accuracy = -(simon_accuracy),
    stroop_RT = -(stroop_RT),
    stroop_accuracy = -(stroop_accuracy),
    selfpaced_gardenpath_target = -(selfpaced_gardenpath_target),
    selfpaced_violation_processing_target = -(selfpaced_violation_processing_target)
  )
```

# 2. Iteration for 0.65 reliability cutoff

## 2.1. Latent factor identification

### 2.1.1 Latent factors in statistical learning domain

Compared to the original model, we removed two indices: -
SEGM_AL_midRT_train [speech segmentation - SEGM RT training -
AGL_ACC_training [artificial grammar learning - AGL ACC training]

**Kaiser-Meyer-Olkin test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_ACC_training,
    SEGM_ACC_TRN3_RND4,
    SEGM_ACC_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
KMO()
```

Indices with unsatisfactory fit for EFA (MSA \< 0.5) were excluded: -
SEGM_ACC_training [speech segmentation - SEGM ACC training] -
SEGM_ACC_TRN3_RND4 [speech segmentation - SEGM ACC TRN3-RND4] -
SEGM_ACC_RND4_REC5 [speech segmentation - SEGM ACC RND4-REC5]

Then, KMO test was repeated.

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
KMO()
```

The reaction time based indices showed poor fit (\< 0.6), however, we
did not omit them for their possible exploratory power. Instead, we
performed two separate analysis with only the reaction time based
variables and the remaining ones.

#### 2.1.1.1. RT variables

**Bartlett's test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates a single factor.

**Exploratory factor analysis**:

```{r echo = FALSE, warning = FALSE}
SL_RT = df %>% 
  select(
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 1,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r echo = FALSE, warning = FALSE}
summary(SL_RT)
```

```{r echo = FALSE, warning = FALSE}
SL_RT$loadings
tidy(SL_RT$uniquenesses)
```

#### 2.1.1.2. Offline SL indices (non-RT indices)

**Bartlett's test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates 2 factors.

**Exploratory factor analysis**:

```{r echo = FALSE, warning = FALSE}
SL_OFF = df %>% 
  select(
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 2,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r echo = FALSE, warning = FALSE}
summary(SL_OFF)
```

```{r echo = FALSE, warning = FALSE}
SL_OFF$loadings
tidy(SL_OFF$uniquenesses)
```

#### 2.1.1.3. Final model for statistical learning variables

Since the 2-factor model showed a complex structure, we proceeded with
hierarchical cluster analysis to reveal potential single-variable
factors.

**Hierarchical cluster analysis**:

```{r echo = FALSE, warning = FALSE}
tree_01 = data.frame(df %>% 
  select(
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  )) %>% 
hclustvar()
```

```{r echo = FALSE, warning = FALSE}
plot(tree_01)
```

```{r echo = FALSE, warning = FALSE}
stab = stability(tree_01, B = 100)
```

Since the Rand criterion shows that there is two local maxima (at 3 and
5) regarding the number of clusters, we selected the greater number to
get a more detailed analysis.

**K-means cluster analysis**:

```{r echo = FALSE, warning = FALSE}
km = data.frame(df %>% 
  select(
    SEGM_RT_TRN3_RND4,
    SEGM_RT_RND4_REC5,
    SEGM_2AFC_bigram,
    SEGM_2AFC_trigram,
    SEGM_production,
    AGL_2AFC_phrase,
    AGL_2AFC_sentence,
    AGL_production
  )) %>% 
kmeansvar(init = 5, nstart = 1000)
```

```{r echo = FALSE, warning = FALSE}
summary(km)
```

In conclusion, we used these 5 clusters of statistical learning
variables to identify latent factors which later serve as predictors in
the SEM models. We tested the robustness of the results with
confirmatory factor analysis.

**Confirmatory factor analysis**:

```{r echo = FALSE, warning = FALSE}
model_SL =
"
  SL_RT =~ SEGM_RT_TRN3_RND4 + SEGM_RT_RND4_REC5
  SL_tribi =~ SEGM_2AFC_bigram + SEGM_2AFC_trigram
  SL_SEGM_prod =~ SEGM_production
  SL_AGL =~ AGL_2AFC_sentence + AGL_production
  SL_phr =~ AGL_2AFC_phrase
"
```

**Model fits for CFA**:

```{r echo = FALSE, warning = FALSE}
model_SL_fit = cfa(model_SL, df, missing = "ML", estimator = "MLR")
summary(model_SL_fit, standardized = TRUE, fit.measures = TRUE)
```

In one case (*SEGM_2AFC_bigram [speech segmentation - SEGM 2AFC
bigram]*), the unexplained variance was high (\>0.65), however we kept
it to assign convergent validity to the SL_trigram variable.

------------------------------------------------------------------------

The final grouping of the variables is:

1.  **SEGM RT**:

-   SEGM_RT_TRN3_RND4 [speech segmentation - SEGM RT TRN3-RND4]
-   SEGM_RT_RND4_REC5 [speech segmentation - SEGM RT RND4-REC5]

2.  **SEGM 2AFC**:

-   SEGM_2AFC_trigram [speech segmentation - SEGM 2AFC trigram]
-   SEGM_2AFC_bigram [speech segmentation - SEGM 2AFC bigram]

3.  **SEGM production**:

-   SEGM_production [speech segmentation - SEGM production]

4.  **AGL**:

-   AGL_2AFC_sentence [artificial grammar learning - AGL 2AFC sentence ]
-   AGL_production [artificial grammar learning - AGL production]

5.  **AGL phrase**:

-   AGL_2AFC_phrase [artificial grammar learning - AGL 2AFC phrase]

### 2.1.2. Latent factors in core cognitive domain

Compared to the original model, we excluded two indices: -
simon_accuracy [simon - accuracy] - simon_RT [simon - RT]

**Kaiser-Meyer-Olkin test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT,
    stroop_accuracy,
  ) %>% 
KMO()
```

Indices with unsatisfactory fit for EFA (MSA \< 0.6) were excluded: -
stroop_accuracy [stroop - accuracy]

Then, KMO test was repeated.

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT
  ) %>% 
KMO()
```

**Bartlett's test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT
  ) %>% 
BARTLETT()
```

**Parallel analysis**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

**Exploratory factor analysis**:

```{r echo = FALSE, warning = FALSE}
COG_EFA = df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 4,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r echo = FALSE, warning = FALSE}
summary(COG_EFA)
```

```{r echo = FALSE, warning = FALSE}
COG_EFA$loadings
tidy(COG_EFA$uniquenesses)
```

------------------------------------------------------------------------

For further analysis, we performed hierarchical variable clustering on
the core cognitive variables as well.

**Hierarchical cluster analysis**:

```{r echo = FALSE, warning = FALSE}
tree_01 = data.frame(df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT
  )) %>% 
hclustvar()
```

```{r echo = FALSE, warning = FALSE}
plot(tree_01)
```

```{r echo = FALSE, warning = FALSE}
stab = stability(tree_01, B=100)
```

Since the Rand criterion shows that there is two local maxima (at 2 and
4) regarding the number of clusters, we chose the bigger number for more
detailed analysis.

**K-means cluster analysis**:

```{r echo = FALSE, warning = FALSE}
km = data.frame(df %>% 
  select(
    perceptual_speed_visual_RT,
    perceptual_speed_auditory_RT,
    perceptual_speed_visual_decision,
    digit_span_forward,
    digit_span_backward,
    nback_1_back_dprime,
    nback_2_back_dprime,
    nback_3_back_dprime,
    stroop_RT
  )) %>% 
kmeansvar(init = 4, nstart = 1000)
```

```{r echo = FALSE, warning = FALSE}
summary(km)
```

------------------------------------------------------------------------

In conclusion, we have 4 clusters of cognitive variables:

1.  **perceptual speed**:

-   perceptual_speed_visual_RT [perceptual speed - visual reaction time]
-   perceptual_speed_auditory_RT [perceptual speed - auditory reaction
    time]
-   perceptual_speed_visual_decision [perceptual speed - visual decision
    time]

2.  **digit span**:

-   digit_span_forward [digit span - forward digit span]
-   digit_span_backward [digit span - backward digit span]

3.  **nback test**:

-   nback_1_back_dprime [n-back - 1-back d-prime]
-   nback_2_back_dprime [n-back - 2-back d-prime]
-   nback_3_back_dprime [n-back - 3-back d-prime]

4.  **cognitive control**:

-   stroop_RT [stroop - RT]

As the following step, we tested whether these factors shows an
appropriate fit using CFA.

**Confirmatory factor analysis**:

```{r echo = FALSE, warning = FALSE}
model_COG =
"
  COG_PS =~ perceptual_speed_visual_RT + perceptual_speed_visual_decision + perceptual_speed_auditory_RT
  COG_DS =~ digit_span_forward + digit_span_backward
  COG_nback =~ nback_1_back_dprime + nback_2_back_dprime + nback_3_back_dprime
  COG_RT =~ stroop_RT
"
```

**Model fits for CFA**:

```{r echo = FALSE, warning = FALSE}
model_COG_fit = cfa(model_COG, df, missing = "ML", estimator = "MLR")
summary(model_COG_fit, standardized = TRUE, fit.measures = TRUE)
```

In two cases the unexplained variance is very high (\>0.65). 1. First,
we excluded the *nback_1_dprime* variable, since it does not carry more
information compared to *nback_2_dprime* and *nback_3_dprime* variables
both theoretically and measurement-wise. 2. Secondly, we split the
*COG_PS factor* into two. (For detail see next section.)

------------------------------------------------------------------------

To split the COG_PS into two factor, we performed two CFA analyses with
nested models (*model_PS_01* and *model_PS_02*).

**First model / Model_PS_01**:

```{r echo = FALSE, warning = FALSE}
model_PS_01 =
"
  PS_01 =~ perceptual_speed_visual_RT + perceptual_speed_visual_decision
  PS_02 =~ perceptual_speed_auditory_RT
  COG_DS =~ digit_span_forward + digit_span_backward
  COG_nback =~ nback_2_back_dprime + nback_3_back_dprime
  COG_stroop =~ stroop_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_PS_01_fit = cfa(model_PS_01, df, missing = "ML", estimator = "MLR")
summary(model_PS_01_fit, standardized = TRUE, fit.measures = TRUE)
```

**Second model / Model_PS_02**:

```{r echo = FALSE, warning = FALSE}
model_PS_02 =
"
  PS_01 =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
  PS_02 =~ perceptual_speed_visual_decision
  COG_DS =~ digit_span_forward + digit_span_backward
  COG_nback =~ nback_2_back_dprime + nback_3_back_dprime
  COG_stroop =~ stroop_RT 
"
```

```{r echo = FALSE, warning = FALSE}
model_PS_02_fit = cfa(model_PS_02, df, missing = "ML", estimator = "MLR")
summary(model_PS_02_fit, standardized = TRUE, fit.measures = TRUE)
```

Based on the two models, we concluded that *model_PS_02* indicates a
better model considering four fit indices (TLI, CFI, SRMR, RMSEA) and
the BIC. So for further analyses, we used the
*perceptual_speed_visual_decision [perceptual speed - visual decision
time]* index as a single-variable latent factor retaining the other two
variables as the *PS factor*.

------------------------------------------------------------------------

The final grouping of the variables is the following structure:

1.  **perceptual speed**:

-   perceptual_speed_visual_RT [perceptual speed - visual reaction time]
-   perceptual_speed_auditory_RT [perceptual speed - auditory reaction
    time]

2.  **visual decision speed**:\

-   perceptual_speed_visual_decision [perceptual speed - visual decision
    time]

3.  **digit span**:

-   digit_span_forward [digit span - forward digit span]
-   digit_span_backward [digit span - backward digit span]

4.  **nback test**:

-   nback_2_back_dprime [n-back - 2-back d-prime]
-   nback_3_back_dprime [n-back - 3-back d-prime]

5.  **Stroop task**:

-   stroop_RT [stroop - RT]

### 2.1.3. Latent factors in language domain

Compared to the original model, we removed three indices: -
predictive_sent_proc_accuracy [predictive sentence processing -
accuracy] - selfpaced_gardenpath_target [self-paced reading - garden
path sentence processing] - pragmatic_comprehension [KOBAK - pragmatic
comprehension]

**Kaiser-Meyer-Olkin test**:

```{r echo = FALSE, warning = FALSE}
df %>% 
  select(
    grammatical_sensitivity,
    one_minute_reading,
    predictive_sent_proc_RT,
    selfpaced_violation_processing_target
  ) %>% 
KMO()
```

Since now, all indices are not in the acceptable range (\> 0.6), we
refrain from performing further factor analysis or cluster analysis, as
the data does not indicate sufficient evidence for a latent grouping to
be established. Therefore, we propose four distinct single-index latent
factors.

The final structure is:

1.  **grammatical sensitivity**

-   grammatical_sensitivity [grammatical sensitivity]

2.  **reading fluency**

-   one_minute_reading [one-minute reading]

3.  **semantic prediction**

-   predictive_sent_proc_RT [predictive sentence processing - RT]

4.  **violation processing**

-   selfpaced_violation_processing_target [self-paced reading - sentence
    violation processing]

### 2.1.4. Summary of the latent factors

**Table 2**

*Latent factors and constituent indices*

| domain | latent factor           | index                             | mean  | SD   | reference value | t     | p      | realiability value |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| SL     | SEGM RT                 | SEGM RT TRN3-RND4                 | 0.17  | 0.15 | 0               | 19.14 | \<.001 | 0.77               |
| SL     | SEGM RT                 | SEGM RT RND4-REC5                 | 0.14  | 0.16 | 0               | 14.24 | \<.001 | 0.78               |
| SL     | SEGM 2AFC               | SEGM 2AFC bigram                  | 0.55  | 0.13 | 0.5             | 6.85  | \<.001 | n.a.               |
| SL     | SEGM 2AFC               | SEGM 2AFC trigram                 | 0.58  | 0.15 | 0.5             | 9.11  | \<.001 | n.a.               |
| SL     | SEGM production         | SEGM production                   | 0.67  | 0.20 | 0.33            | 27.87 | \<.001 | n.a.               |
| SL     | AGL                     | AGL 2AFC sentence                 | 0.68  | 0.16 | 0.5             | 18.02 | \<.001 | n.a.               |
| SL     | AGL                     | AGL 2AFC production               | 0.49  | 0.14 | 0.5             | 18.45 | \<.001 | n.a.               |
| SL     | AGL phrase              | AGL phrase                        | 0.56  | 0.12 | 0.5             | 7.55  | \<.001 | n.a.               |
| LANG   | grammatical sensitivity | grammatical sensitivity           | 0.76  | 0.18 | 0.33            | 48.15 | \<.001 | 0.68               |
| LANG   | violation processing    | sentence violation processing     | -0.24 | 0.31 | 0               | 15.92 | \<.001 | 0.68               |
| LANG   | semantic prediction     | predictive sentence processing RT | 0.57  | 0.32 | 0               | 38.80 | \<.001 | 0.85               |
| LANG   | reading fluency         | one minute reading                | 300   | 90   | n.a.            | n.a.  | n.a.   | n.a.               |
| COG    | perceptual speed        | visual reaction time              | -0.31 | 0.05 | n.a.            | n.a.  | n.a.   | 0.96               |
| COG    | perceptual speed        | auditory reaction time            | -0.30 | 0.05 | n.a.            | n.a.  | n.a.   | 0.96               |
| COG    | visual decision speed   | visual decision time              | -0.98 | 0.24 | n.a.            | n.a.  | n.a.   | 0.95               |
| COG    | digit span              | forward digit span                | 6.57  | 1.79 | n.a.            | n.a.  | n.a.   | n.a.               |
| COG    | digit span              | backward digit span               | 5.43  | 1.96 | n.a.            | n.a.  | n.a.   | n.a.               |
| COG    | nback                   | 2-back d-prime                    | 0.87  | 1.59 | 0               | 11.80 | \<.001 | 0.76               |
| COG    | nback                   | 3-back d-prime                    | -0.45 | 2.30 | 0               | 4.18  | \<.001 | 0.69               |
| COG    | Stroop                  | Stroop RT                         | -0.26 | 0.19 | 0               | 29.97 | \<.001 | 0.77               |

*Note.* In the case of each index, group level performance means,
medians, and results of t-tests testing the learning effect are listed
where applicable. The level of no learning is included in the *reference
value* column for tested indices. Reliability values are listed where
applicable (note that these reliability values are identical to values
in Table 1).

## 2.2. Mediation models (Structural equation modelling)

After the identification of latent factors, we proceeded with assembling
the models for mediation analysis. In each model, there was one
statistical learning variable, all the core cognitive variables, and the
language variables. We specified the models with lavaan syntax. CFA
estimator was Robust Maximum Likelihood method. Missing values were
imputed using Maximum Likelihood method. An optimal model fit was
ascertained based on Comparative Fit Index (CFI) and Tucker-Lewis Index
(TLI) values ranging between 0.90--0.95, and an acceptable model fit was
indicated by a Root Mean Squared Error of Approximation (RMSEA) index
below 0.05 and Standardised Root Mean Square Residual (SRMR) index below
0.08. Inflated indices were expected due to single-item factors.

### 2.2.1. Calculating error variances from reliability indices

```{r echo = FALSE, warning = FALSE}
# Error variances
stroop_e = (1-0.829)*var(df$stroop_RT, na.rm = TRUE)
visdec_e = (1-0.955)*var(df$perceptual_speed_visual_decision, na.rm = TRUE)
menyet_e = (1-0.661)*var(df$grammatical_sensitivity, na.rm = TRUE)
pred_e = (1-0.866)*var(df$predictive_sent_proc_RT, na.rm = TRUE)
```

### 2.2.2. SEGM RT model

```{r echo = FALSE, warning = FALSE}
model_RT = 
"
# statistical learning
SL =~ 1*SEGM_RT_TRN3_RND4 + 1*SEGM_RT_RND4_REC5

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_violation_processing_target
OMR =~ one_minute_reading

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b43*PS_visdec + b44*nback + b45*PS + b46*stroop

# mediator residual covariance
DS ~~ PS_visdec
DS ~~ nback
PS_visdec ~~ nback
DS ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_RT = sem(model_RT, df, missing = "ML", estimator = "MLR")
summary(model_fit_RT, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

### 2.2.3. SEGM 2AFC model

```{r echo = FALSE, warning = FALSE}
model_SEGM_tribi = 
"
# statistical learning
SL =~ SEGM_2AFC_trigram + SEGM_2AFC_bigram

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_violation_processing_target
OMR =~ one_minute_reading

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b43*PS_visdec + b44*nback + b45*PS + b46*stroop

# mediator residual covariance
DS ~~ PS_visdec
DS ~~ nback
PS_visdec ~~ nback
DS ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_SEGM_tribi = sem(model_SEGM_tribi, df, missing = "ML", estimator = "MLR")
summary(model_fit_SEGM_tribi, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

### 2.2.4. SEGM production model

```{r echo = FALSE, warning = FALSE}
model_SEGM_prod = 
"
# statistical learning
SL =~ SEGM_production

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_violation_processing_target
OMR =~ one_minute_reading

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS  + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b43*PS_visdec + b44*nback + b45*PS + b46*stroop

# mediator residual covariance
DS ~~ PS_visdec
DS ~~ nback
PS_visdec ~~ nback
DS ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_SEGM_prod = sem(model_SEGM_prod, df, missing = "ML", estimator = "MLR")
summary(model_fit_SEGM_prod, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

### 2.2.5. AGL model

```{r echo = FALSE, warning = FALSE}
model_AGL = 
"
# statistical learning
SL =~ AGL_2AFC_sentence + AGL_production

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_violation_processing_target
OMR =~ one_minute_reading

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b43*PS_visdec + b44*nback + b45*PS + b46*stroop

# mediator residual covariance
DS ~~ PS_visdec
DS ~~ nback
PS_visdec ~~ nback
DS ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_AGL = sem(model_AGL, df, missing = "ML", estimator = "MLR")
summary(model_fit_AGL, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

### 2.2.6. AGL phrase model

```{r echo = FALSE, warning = FALSE}
model_phr = 
"
# statistical learning
SL =~ AGL_2AFC_phrase

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_violation_processing_target
OMR =~ one_minute_reading

# direct effect: c paths
MENYET ~ c11*SL
pred ~ c21*SL
space ~ c31*SL
OMR ~ c41*SL

# mediator regression: a paths (SL -> cog)
DS ~ a11*SL
PS_visdec ~ a31*SL
nback ~ a41*SL
PS ~ a51*SL
stroop ~ a61*SL

# mediator regression: b paths (cog -> lang)
MENYET ~ b11*DS + b13*PS_visdec + b14*nback + b15*PS + b16*stroop
pred ~ b21*DS + b23*PS_visdec + b24*nback + b25*PS + b26*stroop
space ~ b31*DS + b33*PS_visdec + b34*nback + b35*PS + b36*stroop
OMR ~ b41*DS + b43*PS_visdec + b44*nback + b45*PS + b46*stroop

# mediator residual covariance
DS ~~ PS_visdec
DS ~~ nback
PS_visdec ~~ nback
DS ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_phr = sem(model_phr, df, missing = "ML", estimator = "MLR")
summary(model_fit_phr, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```

### 2.2.7. Unified model

We also tested a unified model With all the variables for calculating
total effect sizes.

```{r echo = FALSE, warning = FALSE}
model_full =
"
# statistical learning
SL_RT =~ SEGM_RT_TRN3_RND4 + SEGM_RT_RND4_REC5
SL_AGL =~ AGL_2AFC_sentence + AGL_production
SL_SEGM_prod =~ SEGM_production
SL_SEGM_tribi =~ SEGM_2AFC_trigram + SEGM_2AFC_bigram
SL_phr =~ AGL_2AFC_phrase

# cognitive
PS =~ perceptual_speed_visual_RT + perceptual_speed_auditory_RT
DS =~ digit_span_forward + digit_span_backward
nback =~ nback_2_back_dprime + nback_3_back_dprime
stroop =~ stroop_RT
PS_visdec =~ perceptual_speed_visual_decision

# language
MENYET =~ grammatical_sensitivity
pred =~ predictive_sent_proc_RT
space =~ selfpaced_violation_processing_target
OMR =~ one_minute_reading

# dependent regression
MENYET ~ b11*DS  + b13*PS_visdec + b14*nback + b15*PS + b16*stroop + c11*SL_RT + c12*SL_AGL + c13*SL_SEGM_prod + c14*SL_SEGM_tribi + c15*SL_phr
pred ~ b21*DS + b23*PS_visdec + b24*nback + b25*PS + b26*stroop + c21*SL_RT + c22*SL_AGL + c23*SL_SEGM_prod + c24*SL_SEGM_tribi + c25*SL_phr
space ~ b31*DS + b33*PS_visdec + b34*nback + b35*PS + b36*stroop + c31*SL_RT + c32*SL_AGL + c33*SL_SEGM_prod + c34*SL_SEGM_tribi + c35*SL_phr
OMR ~ b41*DS + b43*PS_visdec + b44*nback + b45*PS + b46*stroop + c41*SL_RT + c42*SL_AGL + c43*SL_SEGM_prod + c44*SL_SEGM_tribi + c45*SL_phr

# mediator regression
DS ~ a11*SL_RT + a12*SL_AGL + a13*SL_SEGM_prod + a14*SL_SEGM_tribi + a15*SL_phr
PS_visdec ~ a31*SL_RT + a32*SL_AGL + a33*SL_SEGM_prod + a34*SL_SEGM_tribi + a35*SL_phr
nback ~ a41*SL_RT + a42*SL_AGL + a43*SL_SEGM_prod + a44*SL_SEGM_tribi + a45*SL_phr
PS ~ a51*SL_RT + a52*SL_AGL + a53*SL_SEGM_prod + a54*SL_SEGM_tribi + a55*SL_phr
stroop ~ a61*SL_RT + a62*SL_AGL + a63*SL_SEGM_prod + a64*SL_SEGM_tribi + a65*SL_phr

# mediator residual covariance
DS ~~ PS_visdec
DS ~~ nback
PS_visdec ~~ nback
DS ~~ PS
PS_visdec ~~ PS
nback ~~ PS
DS ~~ stroop
PS_visdec ~~ stroop
nback ~~ stroop
PS ~~ stroop

# dependent residual covariance
MENYET ~~ pred
MENYET ~~ space
pred ~~ space
MENYET ~~ OMR
pred ~~ OMR
space ~~ OMR

# effect decomposition
# y1 ~ x1
ind_x1_m1_y1 := a11*b11
ind_x1_m3_y1 := a31*b13
ind_x1_m4_y1 := a41*b14
ind_x1_m5_y1 := a51*b15
ind_x1_m6_y1 := a61*b16
ind_x1_y1 := ind_x1_m1_y1 + ind_x1_m3_y1 + ind_x1_m4_y1 + ind_x1_m5_y1 + ind_x1_m6_y1
tot_x1_y1 := ind_x1_y1 + c11

# y1 ~ x2
ind_x2_m1_y1 := a12*b11
ind_x2_m3_y1 := a32*b13
ind_x2_m4_y1 := a42*b14
ind_x2_m5_y1 := a52*b15
ind_x2_m6_y1 := a62*b16
ind_x2_y1 := ind_x2_m1_y1 + ind_x2_m3_y1 + ind_x2_m4_y1 + ind_x2_m5_y1 + ind_x2_m6_y1
tot_x2_y1 := ind_x2_y1 + c12

# y1 ~ x3
ind_x3_m1_y1 := a13*b11
ind_x3_m3_y1 := a33*b13
ind_x3_m4_y1 := a43*b14
ind_x3_m5_y1 := a53*b15
ind_x3_m6_y1 := a63*b16
ind_x3_y1 := ind_x3_m1_y1 + ind_x3_m3_y1 + ind_x3_m4_y1 + ind_x3_m5_y1 + ind_x3_m6_y1
tot_x3_y1 := ind_x3_y1 + c13

# y1 ~ x4
ind_x4_m1_y1 := a14*b11
ind_x4_m3_y1 := a34*b13
ind_x4_m4_y1 := a44*b14
ind_x4_m5_y1 := a54*b15
ind_x4_m6_y1 := a64*b16
ind_x4_y1 := ind_x4_m1_y1 + ind_x4_m3_y1 + ind_x4_m4_y1 + ind_x4_m5_y1 + ind_x4_m6_y1
tot_x4_y1 := ind_x4_y1 + c14

# y1 ~ x5
ind_x5_m1_y1 := a15*b11
ind_x5_m3_y1 := a35*b13
ind_x5_m4_y1 := a45*b14
ind_x5_m5_y1 := a55*b15
ind_x5_m6_y1 := a65*b16
ind_x5_y1 := ind_x5_m1_y1 + ind_x5_m3_y1 + ind_x5_m4_y1 + ind_x5_m5_y1 + ind_x5_m6_y1
tot_x5_y1 := ind_x5_y1 + c15

# y2 ~ x1
ind_x1_m1_y2 := a11*b21
ind_x1_m3_y2 := a31*b23
ind_x1_m4_y2 := a41*b24
ind_x1_m5_y2 := a51*b25
ind_x1_m6_y2 := a61*b26
ind_x1_y2 := ind_x1_m1_y2 + ind_x1_m3_y2 + ind_x1_m4_y2 + ind_x1_m5_y2 + ind_x1_m6_y2
tot_x1_y2 := ind_x1_y2 + c21

# y2 ~ x2
ind_x2_m1_y2 := a12*b21
ind_x2_m3_y2 := a32*b23
ind_x2_m4_y2 := a42*b24
ind_x2_m5_y2 := a52*b25
ind_x2_m6_y2 := a62*b26
ind_x2_y2 := ind_x2_m1_y2 + ind_x2_m3_y2 + ind_x2_m4_y2 + ind_x2_m5_y2 + ind_x2_m6_y2
tot_x2_y2 := ind_x2_y2 + c22

# y2 ~ x3
ind_x3_m1_y2 := a13*b21
ind_x3_m3_y2 := a33*b23
ind_x3_m4_y2 := a43*b24
ind_x3_m5_y2 := a53*b25
ind_x3_m6_y2 := a63*b26
ind_x3_y2 := ind_x3_m1_y2 + ind_x3_m3_y2 + ind_x3_m4_y2 + ind_x3_m5_y2 + ind_x3_m6_y2
tot_x3_y2 := ind_x3_y2 + c23

# y2 ~ x4
ind_x4_m1_y2 := a14*b21
ind_x4_m3_y2 := a34*b23
ind_x4_m4_y2 := a44*b24
ind_x4_m5_y2 := a54*b25
ind_x4_m6_y2 := a64*b26
ind_x4_y2 := ind_x4_m1_y2 + ind_x4_m3_y2 + ind_x4_m4_y2 + ind_x4_m5_y2 + ind_x4_m6_y2
tot_x4_y2 := ind_x4_y2 + c24

# y2 ~ x5
ind_x5_m1_y2 := a15*b21
ind_x5_m3_y2 := a35*b23
ind_x5_m4_y2 := a45*b24
ind_x5_m5_y2 := a55*b25
ind_x5_m6_y2 := a65*b26
ind_x5_y2 := ind_x5_m1_y2 + ind_x5_m3_y2 + ind_x5_m4_y2 + ind_x5_m5_y2 + ind_x5_m6_y2
tot_x5_y2 := ind_x5_y2 + c25

# y3 ~ x1
ind_x1_m1_y3 := a11*b31
ind_x1_m3_y3 := a31*b33
ind_x1_m4_y3 := a41*b34
ind_x1_m5_y3 := a51*b35
ind_x1_m6_y3 := a61*b36
ind_x1_y3 := ind_x1_m1_y3 + ind_x1_m3_y3 + ind_x1_m4_y3 + ind_x1_m5_y3 + ind_x1_m6_y3
tot_x1_y3 := ind_x1_y3 + c31

# y3 ~ x2
ind_x2_m1_y3 := a12*b31
ind_x2_m3_y3 := a32*b33
ind_x2_m4_y3 := a42*b34
ind_x2_m5_y3 := a52*b35
ind_x2_m6_y3 := a62*b36
ind_x2_y3 := ind_x2_m1_y3 + ind_x2_m3_y3 + ind_x2_m4_y3 + ind_x2_m5_y3 + ind_x2_m6_y3
tot_x2_y3 := ind_x2_y3 + c32

# y3 ~ x3
ind_x3_m1_y3 := a13*b31
ind_x3_m3_y3 := a33*b33
ind_x3_m4_y3 := a43*b34
ind_x3_m5_y3 := a53*b35
ind_x3_m6_y3 := a63*b36
ind_x3_y3 := ind_x3_m1_y3 + ind_x3_m3_y3 + ind_x3_m4_y3 + ind_x3_m5_y3 + ind_x3_m6_y3
tot_x3_y3 := ind_x3_y3 + c33

# y3 ~ x4
ind_x4_m1_y3 := a14*b31
ind_x4_m3_y3 := a34*b33
ind_x4_m4_y3 := a44*b34
ind_x4_m5_y3 := a54*b35
ind_x4_m6_y3 := a64*b36
ind_x4_y3 := ind_x4_m1_y3 + ind_x4_m3_y3 + ind_x4_m4_y3 + ind_x4_m5_y3 + ind_x4_m6_y3
tot_x4_y3 := ind_x4_y3 + c34

# y3 ~ x5
ind_x5_m1_y3 := a15*b31
ind_x5_m3_y3 := a35*b33
ind_x5_m4_y3 := a45*b34
ind_x5_m5_y3 := a55*b35
ind_x5_m6_y3 := a65*b36
ind_x5_y3 := ind_x5_m1_y3 + ind_x5_m3_y3 + ind_x5_m4_y3 + ind_x5_m5_y3 + ind_x5_m6_y3
tot_x5_y3 := ind_x5_y3 + c35

# y4 ~ x1
ind_x1_m1_y4 := a11*b41
ind_x1_m3_y4 := a31*b43
ind_x1_m4_y4 := a41*b44
ind_x1_m5_y4 := a51*b45
ind_x1_m6_y4 := a61*b46
ind_x1_y4 := ind_x1_m1_y4 + ind_x1_m3_y4 + ind_x1_m4_y4 + ind_x1_m5_y4 + ind_x1_m6_y4
tot_x1_y4 := ind_x1_y4 + c41

# y4 ~ x2
ind_x2_m1_y4 := a12*b41
ind_x2_m3_y4 := a32*b43
ind_x2_m4_y4 := a42*b44
ind_x2_m5_y4 := a52*b45
ind_x2_m6_y4 := a62*b46
ind_x2_y4 := ind_x2_m1_y4 + ind_x2_m3_y4 + ind_x2_m4_y4 + ind_x2_m5_y4 + ind_x2_m6_y4
tot_x2_y4 := ind_x2_y4 + c42

# y4 ~ x3
ind_x3_m1_y4 := a13*b41
ind_x3_m3_y4 := a33*b43
ind_x3_m4_y4 := a43*b44
ind_x3_m5_y4 := a53*b45
ind_x3_m6_y4 := a63*b46
ind_x3_y4 := ind_x3_m1_y4 + ind_x3_m3_y4 + ind_x3_m4_y4 + ind_x3_m5_y4 + ind_x3_m6_y4
tot_x3_y4 := ind_x3_y4 + c43

# y4 ~ x4
ind_x4_m1_y4 := a14*b41
ind_x4_m3_y4 := a34*b43
ind_x4_m4_y4 := a44*b44
ind_x4_m5_y4 := a54*b45
ind_x4_m6_y4 := a64*b46
ind_x4_y4 := ind_x4_m1_y4 + ind_x4_m3_y4 + ind_x4_m4_y4 + ind_x4_m5_y4 + ind_x4_m6_y4
tot_x4_y4 := ind_x4_y4 + c44

# y4 ~ x5
ind_x5_m1_y4 := a15*b41
ind_x5_m3_y4 := a35*b43
ind_x5_m4_y4 := a45*b44
ind_x5_m5_y4 := a55*b45
ind_x5_m6_y4 := a65*b46
ind_x5_y4 := ind_x5_m1_y4 + ind_x5_m3_y4 + ind_x5_m4_y4 + ind_x5_m5_y4 + ind_x5_m6_y4
tot_x5_y4 := ind_x5_y4 + c45

# y ~ x(all)

ind_y1 := ind_x1_y1 + ind_x2_y1 + ind_x3_y1 + ind_x4_y1 + ind_x5_y1
tot_y1 := tot_x1_y1 + tot_x2_y1 + tot_x3_y1 + tot_x4_y1 + tot_x5_y1

ind_y2 := ind_x1_y2 + ind_x2_y2 + ind_x3_y2 + ind_x4_y2 + ind_x5_y2
tot_y2 := tot_x1_y2 + tot_x2_y2 + tot_x3_y2 + tot_x4_y2 + tot_x5_y2

ind_y3 := ind_x1_y3 + ind_x2_y3 + ind_x3_y3 + ind_x4_y3 + ind_x5_y3
tot_y3 := tot_x1_y3 + tot_x2_y3 + tot_x3_y3 + tot_x4_y3 + tot_x5_y3

ind_y4 := ind_x1_y4 + ind_x2_y4 + ind_x3_y4 + ind_x4_y4 + ind_x5_y4
tot_y4 := tot_x1_y4 + tot_x2_y4 + tot_x3_y4 + tot_x4_y4 + tot_x5_y4

dir_y1 := tot_y1 - ind_y1
dir_y2 := tot_y2 - ind_y2
dir_y3 := tot_y3 - ind_y3
dir_y4 := tot_y4 - ind_y4

# single indicator factors

stroop_RT ~~ 0.152*stroop_RT
perceptual_speed_visual_decision ~~ 0.043*perceptual_speed_visual_decision
grammatical_sensitivity ~~ 0.334*grammatical_sensitivity
predictive_sent_proc_RT ~~ 0.136*predictive_sent_proc_RT
"
```

```{r echo = FALSE, warning = FALSE}
model_fit_full = sem(model_full, df, missing = "ML", estimator = "MLR")
summary(model_fit_full, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE)
```
### 2.2.8. Summary of mediation models

**Table 3**

*Fit indices of mediation models*

| model           | CFI  | R-CFI | TLI  | R-TLI | R-RMSEA | SRMR |
|-----------------|------|-------|------|-------|---------|------|
| SEGM RT         | .986 | .972  | .966 | .934  | .048    | .026 |
| SEGM 2AFC       | .999 | .999  | .999 | .999  | .000    | .010 |
| SEGM production | .996 | .999  | .989 | .996  | .012    | .016 |
| AGL             | .993 | .996  | .982 | .990  | .018    | .022 |
| AGL phrase      | .994 | .997  | .984 | .991  | .016    | .018 |
| SL overall      | .992 | .985  | .983 | .966  | .028    | .030 |

*Note.* CFI: = Comparative Fit Index, R-CFI: = Robust Comparative Fit
Index, TLI: = Tucker-Lewis Index, R-TLI: = Robust Tucker-Lewis Index,
R-RMSEA: = Robust Root Mean Square Error of Approximation, SRMR: =
Standardised Root Mean Square Residual.

**Table 4**

*Reliability values and average variance extracted (AVE) for latent
variables*

```{r echo = FALSE, warning = FALSE}
reliability(model_fit_RT)
reliability(model_fit_SEGM_tribi)
reliability(model_fit_SEGM_prod)
reliability(model_fit_AGL)
reliability(model_fit_phr)
reliability(model_fit_full)
```

**Table 5**

*Mediation models*

| predictor       | outcome                 | direct effect | indirect effect | total effect |
|---------------|---------------|---------------|---------------|---------------|
| SEGM RT         | grammatical sensitivity | -.115         | .114\*          | -.001        |
|                 | semantic prediction     | -.071         | .068\*          | -.003        |
|                 | violation processing    | .046          | .128\*\*        | .174         |
|                 | reading fluency         | .065          | .094\*          | .159\*       |
|                 | pragmatic comprehension | .130          | .101\*          | .231\*\*     |
| SEGM 2AFC       | grammatical sensitivity | .394\*        | .099\*          | .493\*\*     |
|                 | semantic prediction     | .099          | .074            | .174         |
|                 | violation processing    | .033          | .166\*\*        | .198         |
|                 | reading fluency         | -.151         | .157\*\*        | .005         |
|                 | pragmatic comprehension | .144          | .118\*          | .262\*       |
| SEGM production | grammatical sensitivity | .304\*\*      | .145\*\*        | .450\*\*\*   |
|                 | semantic prediction     | -.047         | .136\*\*        | .088         |
|                 | violation processing    | -.062         | .193\*\*        | .132         |
|                 | reading fluency         | .114          | .111\*          | .225\*\*\*   |
|                 | pragmatic comprehension | .258\*\*      | .138\*\*        | .396\*\*\*   |
| AGL             | grammatical sensitivity | .143          | .233\*          | .376\*\*\*   |
|                 | semantic prediction     | .156          | .114            | .269\*\*     |
|                 | violation processing    | .077          | .213\*          | .290\*       |
|                 | reading fluency         | .259          | .072            | .331\*\*\*   |
|                 | pragmatic comprehension | .446\*        | .069            | .515\*\*\*   |
| AGL phrase      | grammatical sensitivity | -.018         | .125\*\*        | .106         |
|                 | semantic prediction     | .031          | .062\*          | .093         |
|                 | violation processing    | .128          | .071            | .199         |
|                 | reading fluency         | -.072         | .073\*          | .002         |
|                 | pragmatic comprehension | .109          | .081\*          | .190\*       |
| SL overall      | grammatical sensitivity | .336          | .215            | .551\*\*\*   |
|                 | semantic prediction     | .079          | .174            | .253\*\*     |
|                 | violation processing    | .152          | .340\*          | .491\*\*     |
|                 | reading fluency         | .191          | .117            | .308\*\*     |
|                 | pragmatic comprehension | .625          | -.009           | .617\*\*\*   |

*Note.* \*: p \< .05, \*\*: p \< .01, \*\*\*: p \< .001