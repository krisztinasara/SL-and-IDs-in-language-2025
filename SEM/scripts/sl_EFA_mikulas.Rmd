---
title: "Exploratory Factor Analysis"
output: html_notebook
---

```{r}
library(tidyverse)
library(broom)
library(psych)
library(EFAtools)
library(lavaan)
library(semTools)
library(ClustOfVar)
```

data:

```{r}
df = read.csv("~/GitHub/lendulet_language_SL/SEM/dataframes/filtered_SEM_df.csv")
```

# 1. Statistical learning variables

Kaiser-Meyer-Olkin test

```{r}
df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
    SEGM_AL_ACC_train,
    SEGM_AL_ACC_TRN3_RND4,
    SEGM_AL_ACC_RND4_REC5,
    AGL_ACC_train,
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
KMO()
```
Consequently, we omitted those variables which showed unacceptable for EFA (<0.5):
 - SEGM_AL_ACC_train
 - SEGM_AL_ACC_TRN3_RND4
 - SEGM_AL_ACC_RND4_REC5
 - AGL_ACC_train
 
```{r}
df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
KMO()
```
The reaction time based variables show poor fit (<0.6), however for their possible exploratory power we did not omit them. Instead we performed two separate analysis with only the RT variables and the others.

# 1.1. RT variables

Bartlett's test

```{r}
df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
  ) %>% 
BARTLETT()
```

```{r}
df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```
Parallel indicates a single factor.

```{r}
SL_RT = df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 1,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r}
summary(SL_RT)
```

```{r}
SL_RT$loadings
tidy(SL_RT$uniquenesses)
```

# 1.2. Offline variables

Bartlett's test

```{r}
df %>% 
  select(
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
BARTLETT()
```

```{r}
df %>% 
  select(
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```
Parallel analysis indicates 2 factors.

```{r}
SL_OFF = df %>% 
  select(
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 2,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r}
summary(SL_OFF)
```
```{r}
SL_OFF$loadings
tidy(SL_OFF$uniquenesses)
```

Since the 2-factor model were showed a complex structure, we proceeded with cluster analysis to reveal single variable factors.

```{r}
tree_01 = df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
hclustvar()
```

```{r}
plot(tree_01)
```

```{r}
stab = stability(tree_01, B=100)
```
Since the Rand criterion shows that there is two local maximum (at 3 and 5) regarding the number of clusters, we chose the bigger number for more detailed analysis.

```{r}
km = df %>% 
  select(
    SEGM_AL_medRT_train,
    SEGM_AL_medRT_TRN3_RND4,
    SEGM_AL_medRT_RND4_REC5,
    SEGM_AL_2AFC_bigram,
    SEGM_AL_2AFC_trigram,
    SEGM_AL_SEGM_prod_data,
    AGL_2AFC_phr,
    AGL_2AFC_sent,
    AGL_prod
  ) %>% 
kmeansvar(init = 5, nstart = 1000)
```

```{r}
summary(km)
```

In conclusion, we used these 5 clusters of statistical learning variables to identify latent variables which serve as predictors in the SEM models. We tested the structure with cfa.

```{r}
model_SL =
"
  SL_RT =~ SEGM_AL_medRT_train + SEGM_AL_medRT_TRN3_RND4 + SEGM_AL_medRT_RND4_REC5
  SL_tribi =~ SEGM_AL_2AFC_bigram + SEGM_AL_2AFC_trigram
  SL_SEGM_prod =~ SEGM_AL_SEGM_prod_data
  SL_AGL =~ AGL_2AFC_sent + AGL_prod
  SL_phr =~ AGL_2AFC_phr
"
```

```{r}
model_SL_fit = cfa(model_SL, df, missing = "ML", estimator = "MLR")
summary(model_SL_fit, standardized = TRUE, fit.measures = TRUE)
```

In one case (SEGM_bigram), the unexplained variance was high (>0.65), however we kept it to assign convergent reliability to the trigram variable. We further discarded the SEGM_AL_midRT_train variable to solve the Heywood case of this factor.

The final grouping of the variables is:
  
  1.SL_RT
   - SEGM_AL_medRT_TRN3_RND4 
   - SEGM_AL_medRT_RND4_REC5
  
  2. SL_tribi
   - SEGM_AL_2AFC_trigram
   - SEGM_AL_2AFC_bigram (for reliability purpose)
   
  3. SL_SEGM_prod
   - SEGM_AL_SEGM_prod_data
   
  4. SL_AGL_sent
   - AGL_2AFC_sent
   - AGL_prod
   
  5. SL_AGL_phr
   - AGL_2AFC_phr

# 2. General cognitive variables

Kaiser-Meyer-Olkin test

```{r}
df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    stroop_ACC_score,
    simon_RT_score,
    simon_ACC_score
  ) %>% 
KMO()
```
We omitted variables due to their low value (<0.6):
 - simon_ACC_score
 - stroop_ACC_score


```{r}
df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
KMO()
```
```{r}
df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
BARTLETT()
```

```{r}
df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```
```{r}
COG_EFA = df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 4,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r}
summary(COG_EFA)
```
```{r}
COG_EFA$loadings
tidy(COG_EFA$uniquenesses)
```

For further analysis, we performed variable clustering on the general cognitive variables as well.

```{r}
tree_01 = df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
hclustvar()
```

```{r}
plot(tree_01)
```

```{r}
stab = stability(tree_01, B=100)
```
Since the Rand criterion shows that there is two local maximum (at 2 and 4) regarding the number of clusters, we chose the bigger number for more detailed analysis.

```{r}
km = df %>% 
  select(
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med,
    digit_span_forward_span,
    digit_span_backward_span,
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime,
    stroop_RT_score,
    simon_RT_score,
  ) %>% 
kmeansvar(init = 4, nstart = 1000)
```

```{r}
summary(km)
```

In conclusion, we have 4 clusters of cognitive variables:
  
  1.Processing Speed:
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med,
    PROC_SPEED_vis_dec_RT_med
  
  2.Digit Span:
    digit_span_forward_span,
    digit_span_backward_span
  
  3.NBACK test:
    n_back_nback_1_dprime,
    n_back_nback_2_dprime,
    n_back_nback_3_dprime
    
  4.Reaction Time:
    stroop_RT_score,
    simon_RT_score
    
Next we test whether these factors have good fit using cfa.

```{r}
model_COG =
"
  COG_PS =~ PROC_SPEED_vis_RT_med + PROC_SPEED_vis_dec_RT_med + PROC_SPEED_ac_RT_med
  COG_DS =~ digit_span_forward_span + digit_span_backward_span
  COG_nback =~ n_back_nback_1_dprime + n_back_nback_2_dprime + n_back_nback_3_dprime
  COG_RT =~ stroop_RT_score + simon_RT_score
"
```

```{r}
model_COG_fit = cfa(model_COG, df, missing = "ML", estimator = "MLR")
summary(model_COG_fit, standardized = TRUE, fit.measures = TRUE)
```

In three cases the unexplained variance is very high (>0.65). 
  1. First, we omitted the nback_1_dprime variable, since it does not carry more information compared to nback_2_dprime and nback_3_dprime variables both theoretically and measurement-wise.
  2. Secondly, we split the COG_RT factor into two, since the stroop and simon tasks have potentially different exploratoriy power in the model.
  3. Thirdly, we split the COG_PS factor into two. See next section.
  
To split the COG_PS into two factor, we performed two cfa analyses with nested models (model_PS_01 and model_PS_02).
  
```{r}
model_PS_01 =
"
  PS_01 =~ PROC_SPEED_vis_RT_med + PROC_SPEED_vis_dec_RT_med
  PS_02 =~ PROC_SPEED_ac_RT_med
  COG_DS =~ digit_span_forward_span + digit_span_backward_span
  COG_nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
  COG_stroop =~ stroop_RT_score 
  COG_simon =~ simon_RT_score
"
```

```{r}
model_PS_01_fit = cfa(model_PS_01, df, missing = "ML", estimator = "MLR")
summary(model_PS_01_fit, standardized = TRUE, fit.measures = TRUE)
```

```{r}
model_PS_02 =
"
  PS_01 =~ PROC_SPEED_vis_RT_med + PROC_SPEED_ac_RT_med
  PS_02 =~ PROC_SPEED_vis_dec_RT_med
  COG_DS =~ digit_span_forward_span + digit_span_backward_span
  COG_nback =~ n_back_nback_2_dprime + n_back_nback_3_dprime
  COG_stroop =~ stroop_RT_score 
  COG_simon =~ simon_RT_score
"
```

```{r}
model_PS_02_fit = cfa(model_PS_02, df, missing = "ML", estimator = "MLR")
summary(model_PS_02_fit, standardized = TRUE, fit.measures = TRUE)
```

Based on the models, we concluded that model_PS_02 indicates a better model considering four fit indeces (TLI, CFI, SRMR, RMSEA) and the BIC. So for further analysis we used the PS_visdec variable as a single-variable latent factor retaining the other two variables as the PS factor.
The final grouping of the variables is the following:

  1.Processing Speed:
    PROC_SPEED_vis_RT_med,
    PROC_SPEED_ac_RT_med
    
  2.Processing Speed Visual Decision:  
    PROC_SPEED_vis_dec_RT_med
  
  3.Digit Span:
    digit_span_forward_span,
    digit_span_backward_span
  
  4.NBACK test:
    n_back_nback_2_dprime,
    n_back_nback_3_dprime
    
  5.Stroop task:
    stroop_RT_score
    
  6.Simon task
    simon_RT_score


# 3. Language related measures

Kaiser-Meyer-Olkin test

```{r}
df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    predictive_ACC_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
KMO()
```
We omitted those variables which showed unacceptable KMO values (<0.5)
 - predictive_ACC_score

```{r}
df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
KMO()
```
All variables are now in the acceptable range (>0.6), and the predictive_RT_score is also near.

```{r}
df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
    fa.parallel(fm = "ml", fa = "fa")
```

Parallel analysis indicates 2 factors emerging.

```{r}
LANG_EFA = df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
  # maximum likelihood
  fa(
    nfactors = 2,
    fm = "ml",
    rotate = "Promax",
    use = "pairwise",
    cor = "cor",
    scores = "regression",
    missing = TRUE,
    impute = "mean"
    )
```

```{r}
summary(LANG_EFA)
```
```{r}
LANG_EFA$loadings
tidy(LANG_EFA$uniquenesses)
```

For further analysis, we performed variable clustering on the language variables as well.

```{r}
tree_01 = df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean
  ) %>% 
hclustvar()
```

```{r}
plot(tree_01)
```

```{r}
stab = stability(tree_01, B=100)
```

```{r}
km = df %>% 
  select(
    MENYET_mean_ACC_all,
    OMR_read_syls,
    predictive_RT_score,
    selfpaced_target_RT_diff_GP,
    selfpaced_target_RT_diff_sertes,
    trog_pragm_ACC_mean

  ) %>% 
kmeansvar(init = 5, nstart = 1000)
```

```{r}
summary(km)
```

Next we test whether these factors have good fit using cfa.

```{r}
model_LANG =
"
  LANG_MENYET =~ MENYET_mean_ACC_all
  LANG_OMR =~ OMR_read_syls
  LANG_pred =~ predictive_RT_score
  LANG_space =~ selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
  LANG_TROG =~ trog_pragm_ACC_mean
"
```

```{r}
model_LANG_fit = cfa(model_LANG, df, missing = "ML", estimator = "MLR")
summary(model_LANG_fit, standardized = TRUE, fit.measures = TRUE)
```

In conclusion, we used these 5 clusters of language related variables to identify latent variables which serve as outcome in the SEM models.

  1. MENYET_mean_ACC_all,
  2. OMR_read_syls,
  3. predictive_RT_score,
  4. selfpaced_target_RT_diff_GP + selfpaced_target_RT_diff_sertes
  5. trog_pragm_ACC_mean